{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calibration using OPENCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_disparity(left_img_path, right_img_path):\n",
    "    \"\"\"Compute disparity map using SGBM\"\"\"\n",
    "    # Read images\n",
    "    left_img = cv2.imread(left_img_path)\n",
    "    right_img = cv2.imread(right_img_path)\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    left_gray = cv2.cvtColor(left_img, cv2.COLOR_BGR2GRAY)\n",
    "    right_gray = cv2.cvtColor(right_img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Create SGBM matcher\n",
    "    window_size = 3\n",
    "    min_disp = 0\n",
    "    num_disp = 160\n",
    "    \n",
    "    stereo = cv2.StereoSGBM_create(\n",
    "        minDisparity=min_disp,\n",
    "        numDisparities=num_disp,\n",
    "        blockSize=window_size,\n",
    "        P1=8 * 3 * window_size**2,\n",
    "        P2=32 * 3 * window_size**2,\n",
    "        disp12MaxDiff=1,\n",
    "        uniquenessRatio=10,\n",
    "        speckleWindowSize=100,\n",
    "        speckleRange=32\n",
    "    )\n",
    "    \n",
    "    # Compute disparity\n",
    "    disparity = stereo.compute(left_gray, right_gray)\n",
    "    \n",
    "    # Normalize disparity for visualization\n",
    "    disparity = cv2.normalize(disparity, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "    \n",
    "    return disparity\n",
    "\n",
    "def compute_calibration_errors(objpoints, imgpoints, mtx, dist, rvecs, tvecs):\n",
    "    \"\"\"\n",
    "    Compute reprojection error for camera intrinsics calibration\n",
    "    Returns mean error and per-image errors\n",
    "    \"\"\"\n",
    "    total_error = 0\n",
    "    errors = []\n",
    "    \n",
    "    for i in range(len(objpoints)):\n",
    "        # Project 3D points to image plane\n",
    "        projected_points, _ = cv2.projectPoints(objpoints[i], rvecs[i], tvecs[i], mtx, dist)\n",
    "        \n",
    "        # Calculate error\n",
    "        error = cv2.norm(imgpoints[i], projected_points, cv2.NORM_L2) / len(projected_points)\n",
    "        errors.append(error)\n",
    "        total_error += error\n",
    "        \n",
    "    mean_error = total_error / len(objpoints)\n",
    "    return mean_error, errors\n",
    "\n",
    "def compute_epipolar_error(left_imgpoints, right_imgpoints, F):\n",
    "    \"\"\"\n",
    "    Compute epipolar alignment error using Sampson distance\n",
    "    Returns mean error and per-point errors\n",
    "    \"\"\"\n",
    "    total_error = 0\n",
    "    errors = []\n",
    "    \n",
    "    for left_points, right_points in zip(left_imgpoints, right_imgpoints):\n",
    "        # Convert points to homogeneous coordinates\n",
    "        left_points = left_points.reshape(-1, 2)\n",
    "        right_points = right_points.reshape(-1, 2)\n",
    "        \n",
    "        left_homog = np.column_stack((left_points, np.ones(len(left_points))))\n",
    "        right_homog = np.column_stack((right_points, np.ones(len(right_points))))\n",
    "        \n",
    "        # Compute Sampson distance for each point pair\n",
    "        for left_pt, right_pt in zip(left_homog, right_homog):\n",
    "            # Calculate terms for Sampson distance\n",
    "            Fx = np.dot(F, left_pt)\n",
    "            Ftx = np.dot(F.T, right_pt)\n",
    "            \n",
    "            # Sampson distance\n",
    "            numerator = (np.dot(right_pt, np.dot(F, left_pt)))**2\n",
    "            denominator = Fx[0]**2 + Fx[1]**2 + Ftx[0]**2 + Ftx[1]**2\n",
    "            \n",
    "            error = numerator / denominator\n",
    "            errors.append(np.sqrt(error))\n",
    "            total_error += error\n",
    "    \n",
    "    mean_error = np.mean(errors)\n",
    "    return mean_error, errors\n",
    "\n",
    "def plot_calibration_errors(left_errors, right_errors, epipolar_errors):\n",
    "    \"\"\"Plot calibration error distributions\"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Intrinsics reprojection error\n",
    "    plt.subplot(131)\n",
    "    plt.hist(left_errors, bins=20, alpha=0.5, label='Left Camera', color='blue')\n",
    "    plt.hist(right_errors, bins=20, alpha=0.5, label='Right Camera', color='red')\n",
    "    plt.xlabel('Reprojection Error (pixels)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Intrinsics Reprojection Error')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Epipolar error\n",
    "    plt.subplot(132)\n",
    "    plt.hist(epipolar_errors, bins=20, color='green')\n",
    "    plt.xlabel('Sampson Distance (pixels)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Epipolar Error Distribution')\n",
    "    \n",
    "    # Box plot for comparison\n",
    "    plt.subplot(133)\n",
    "    plt.boxplot([left_errors, right_errors, epipolar_errors], \n",
    "                labels=['Left', 'Right', 'Epipolar'])\n",
    "    plt.ylabel('Error (pixels)')\n",
    "    plt.title('Error Comparison')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def calibrate_single_camera(images_path, board_size=(5,7), debug=False):\n",
    "    \"\"\"Calibrate a single camera using checkerboard images\"\"\"\n",
    "    objp = np.zeros((board_size[0] * board_size[1], 3), np.float32)\n",
    "    objp[:,:2] = np.mgrid[0:board_size[0], 0:board_size[1]].T.reshape(-1,2)\n",
    "    \n",
    "    objpoints = []\n",
    "    imgpoints = []\n",
    "    images = sorted(glob.glob(str(images_path / '*.jpg')))\n",
    "    valid_images = []\n",
    "    \n",
    "    print(f\"Processing {len(images)} images...\")\n",
    "    \n",
    "    for idx, fname in enumerate(images):\n",
    "        img = cv2.imread(fname)\n",
    "        if img is None:\n",
    "            print(f\"Failed to read image: {fname}\")\n",
    "            continue\n",
    "            \n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        ret, corners = cv2.findChessboardCorners(gray, board_size, None)\n",
    "        \n",
    "        if ret:\n",
    "            objpoints.append(objp)\n",
    "            corners2 = cv2.cornerSubPix(gray, corners, (11,11), (-1,-1),\n",
    "                                      (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001))\n",
    "            imgpoints.append(corners2)\n",
    "            valid_images.append(fname)\n",
    "        else:\n",
    "            print(f\"Failed to find corners in image: {fname}\")\n",
    "    \n",
    "    if len(objpoints) < 10:\n",
    "        raise ValueError(f\"Too few valid images ({len(objpoints)}). Need at least 10 for good calibration.\")\n",
    "    \n",
    "    print(f\"Successfully processed {len(objpoints)} out of {len(images)} images\")\n",
    "    \n",
    "    # Calibrate camera\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(\n",
    "        objpoints, imgpoints, gray.shape[::-1], None, None,\n",
    "        flags=cv2.CALIB_RATIONAL_MODEL)\n",
    "    \n",
    "    # Calculate initial reprojection error\n",
    "    mean_error = 0\n",
    "    for i in range(len(objpoints)):\n",
    "        imgpoints2, _ = cv2.projectPoints(objpoints[i], rvecs[i], tvecs[i], mtx, dist)\n",
    "        error = cv2.norm(imgpoints[i], imgpoints2, cv2.NORM_L2)/len(imgpoints2)\n",
    "        mean_error += error\n",
    "    \n",
    "    print(f\"Initial mean reprojection error: {mean_error/len(objpoints):.6f}\")\n",
    "    \n",
    "    return mtx, dist, objpoints, imgpoints, rvecs, tvecs, valid_images\n",
    "\n",
    "def stereo_calibrate(left_path, right_path, left_mtx, left_dist, right_mtx, right_dist, board_size=(5,7)):\n",
    "    \"\"\"Calibrate stereo camera setup\"\"\"\n",
    "    objp = np.zeros((board_size[0] * board_size[1], 3), np.float32)\n",
    "    objp[:,:2] = np.mgrid[0:board_size[0], 0:board_size[1]].T.reshape(-1,2)\n",
    "    \n",
    "    objpoints = []\n",
    "    left_imgpoints = []\n",
    "    right_imgpoints = []\n",
    "    \n",
    "    left_images = sorted(glob.glob(str(left_path / '*.jpg')))\n",
    "    right_images = sorted(glob.glob(str(right_path / '*.jpg')))\n",
    "    \n",
    "    if len(left_images) != len(right_images):\n",
    "        raise ValueError(\"Number of left and right images must match\")\n",
    "    \n",
    "    print(f\"Processing {len(left_images)} stereo pairs...\")\n",
    "    \n",
    "    for left_fname, right_fname in zip(left_images, right_images):\n",
    "        left_img = cv2.imread(left_fname)\n",
    "        right_img = cv2.imread(right_fname)\n",
    "        \n",
    "        if left_img is None or right_img is None:\n",
    "            print(f\"Failed to read images: {left_fname} or {right_fname}\")\n",
    "            continue\n",
    "            \n",
    "        left_gray = cv2.cvtColor(left_img, cv2.COLOR_BGR2GRAY)\n",
    "        right_gray = cv2.cvtColor(right_img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        left_ret, left_corners = cv2.findChessboardCorners(left_gray, board_size, None)\n",
    "        right_ret, right_corners = cv2.findChessboardCorners(right_gray, board_size, None)\n",
    "        \n",
    "        if left_ret and right_ret:\n",
    "            objpoints.append(objp)\n",
    "            \n",
    "            left_corners2 = cv2.cornerSubPix(left_gray, left_corners, (11,11), (-1,-1),\n",
    "                                           (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001))\n",
    "            right_corners2 = cv2.cornerSubPix(right_gray, right_corners, (11,11), (-1,-1),\n",
    "                                            (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001))\n",
    "            \n",
    "            left_imgpoints.append(left_corners2)\n",
    "            right_imgpoints.append(right_corners2)\n",
    "    \n",
    "    if len(objpoints) < 10:\n",
    "        raise ValueError(f\"Too few valid stereo pairs ({len(objpoints)}). Need at least 10.\")\n",
    "    \n",
    "    print(f\"Using {len(objpoints)} valid stereo pairs for calibration\")\n",
    "    \n",
    "    # Stereo calibration with better flags\n",
    "    flags = (cv2.CALIB_FIX_INTRINSIC + \n",
    "            cv2.CALIB_USE_INTRINSIC_GUESS +\n",
    "            cv2.CALIB_RATIONAL_MODEL)\n",
    "    \n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 1e-5)\n",
    "    \n",
    "    ret, _, _, _, _, R, T, E, F = cv2.stereoCalibrate(\n",
    "        objpoints, left_imgpoints, right_imgpoints,\n",
    "        left_mtx, left_dist, right_mtx, right_dist,\n",
    "        left_gray.shape[::-1], criteria=criteria, flags=flags)\n",
    "    \n",
    "    print(f\"Stereo calibration RMS error: {ret}\")\n",
    "    \n",
    "    return R, T, E, F, objpoints, left_imgpoints, right_imgpoints\n",
    "\n",
    "def main():\n",
    "    left_path = Path(\"checkerboards/left eye checkerboard 5x7 B14 R3\")\n",
    "    right_path = Path(\"checkerboards/right eye checkerboard 5x7 B14 R3\")\n",
    "    \n",
    "    # Verify paths exist\n",
    "    if not left_path.exists() or not right_path.exists():\n",
    "        raise ValueError(\"Checkerboard image directories not found\")\n",
    "    \n",
    "    # Print number of images found\n",
    "    left_images = sorted(glob.glob(str(left_path / '*.jpg')))\n",
    "    right_images = sorted(glob.glob(str(right_path / '*.jpg')))\n",
    "    print(f\"Found {len(left_images)} left images and {len(right_images)} right images\")\n",
    "    \n",
    "    # Calibrate individual cameras\n",
    "    print(\"\\nCalibrating left camera...\")\n",
    "    left_mtx, left_dist, left_objpoints, left_imgpoints, left_rvecs, left_tvecs, left_valid = calibrate_single_camera(left_path)\n",
    "    \n",
    "    print(\"\\nCalibrating right camera...\")\n",
    "    right_mtx, right_dist, right_objpoints, right_imgpoints, right_rvecs, right_tvecs, right_valid = calibrate_single_camera(right_path)\n",
    "    \n",
    "    # Print camera matrices and distortion coefficients\n",
    "    print(\"\\nLeft camera matrix:\\n\", left_mtx)\n",
    "    print(\"Left distortion coefficients:\", left_dist.ravel())\n",
    "    print(\"\\nRight camera matrix:\\n\", right_mtx)\n",
    "    print(\"Right distortion coefficients:\", right_dist.ravel())\n",
    "    \n",
    "    # Stereo calibration\n",
    "    print(\"\\nPerforming stereo calibration...\")\n",
    "    R, T, E, F, stereo_objpoints, stereo_left_imgpoints, stereo_right_imgpoints = stereo_calibrate(\n",
    "        left_path, right_path, left_mtx, left_dist, right_mtx, right_dist)\n",
    "    \n",
    "    # Compute errors\n",
    "    left_mean_error, left_errors = compute_calibration_errors(\n",
    "        left_objpoints, left_imgpoints, left_mtx, left_dist, left_rvecs, left_tvecs)\n",
    "    right_mean_error, right_errors = compute_calibration_errors(\n",
    "        right_objpoints, right_imgpoints, right_mtx, right_dist, right_rvecs, right_tvecs)\n",
    "    \n",
    "    print(\"\\nCalibration Results:\")\n",
    "    print(f\"Left camera reprojection error: {left_mean_error:.6f} pixels\")\n",
    "    print(f\"Right camera reprojection error: {right_mean_error:.6f} pixels\")\n",
    "    \n",
    "    mean_epipolar_error, epipolar_errors = compute_epipolar_error(\n",
    "        stereo_left_imgpoints, stereo_right_imgpoints, F)\n",
    "    print(f\"Mean epipolar error: {mean_epipolar_error:.6f} pixels\")\n",
    "    print(f\"Min epipolar error: {np.min(epipolar_errors):.6f} pixels\")\n",
    "    print(f\"Max epipolar error: {np.max(epipolar_errors):.6f} pixels\")\n",
    "    \n",
    "    # Plot error distributions\n",
    "    plot_calibration_errors(left_errors, right_errors, epipolar_errors)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point Cloud Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single image using Depth Anything Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from transformers import AutoImageProcessor, AutoModelForDepthEstimation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"Get the best available device (MPS, CUDA, or CPU)\"\"\"\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "def rectify_images(image_path, camera_matrix, dist_coeffs):\n",
    "    \"\"\"Rectify and undistort a single image using camera calibration parameters\"\"\"\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Could not load image at {image_path}\")\n",
    "        \n",
    "    # Get image size\n",
    "    image_size = (image.shape[1], image.shape[0])\n",
    "    \n",
    "    # Undistort image using camera parameters\n",
    "    undistorted = cv2.undistort(image, camera_matrix, dist_coeffs, None, camera_matrix)\n",
    "\n",
    "    # Convert BGR to RGB\n",
    "    undistorted_rgb = cv2.cvtColor(undistorted, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Save undistorted image with original name + rectified\n",
    "    image_name = image_path.split(\"/\")[-1]\n",
    "    base_name = image_name.rsplit(\".\", 1)[0]\n",
    "    cv2.imwrite(f\"{base_name}_rectified.jpg\", undistorted)\n",
    "    \n",
    "    return undistorted_rgb\n",
    "    \n",
    "def load_depth_model(model_name=\"depth-anything/Depth-Anything-V2-Metric-Indoor-Large-hf\"):\n",
    "    \"\"\"Load the Depth Anything V2 model and processor with MPS acceleration\"\"\"\n",
    "    device = get_device()\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    image_processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "    model = AutoModelForDepthEstimation.from_pretrained(model_name)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model, image_processor, device\n",
    "\n",
    "def estimate_depth(image_path, model, processor, device, camera_matrix, dist_coeffs):\n",
    "    \"\"\"Estimate depth from image using Depth Anything V2\"\"\"\n",
    "    # First rectify the image\n",
    "    image_rgb = rectify_images(image_path, camera_matrix, dist_coeffs)\n",
    "    \n",
    "    # Convert to PIL Image\n",
    "    image_pil = Image.fromarray(image_rgb)\n",
    "\n",
    "    # Process image for depth estimation\n",
    "    inputs = processor(images=image_pil, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move inputs to device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predicted_depth = outputs.predicted_depth\n",
    "\n",
    "    # Interpolate to original size\n",
    "    prediction = torch.nn.functional.interpolate(\n",
    "        predicted_depth.unsqueeze(1),\n",
    "        size=(image_rgb.shape[0], image_rgb.shape[1]),\n",
    "        mode=\"bicubic\",\n",
    "        align_corners=False,\n",
    "    ).squeeze()\n",
    "\n",
    "    # Move back to CPU for numpy conversion\n",
    "    prediction = prediction.cpu()\n",
    "    return image_rgb, prediction.numpy()\n",
    "\n",
    "def create_point_cloud(rgb_image, depth_map, fx=None, fy=None, cx=None, cy=None):\n",
    "    \"\"\"Convert RGB-D to point cloud using camera intrinsics\"\"\"\n",
    "    # Get image dimensions\n",
    "    height, width = depth_map.shape\n",
    "    \n",
    "    camera_intrinsics = np.array([\n",
    "        [1.06936178e+03, 0.00000000e+00, 5.55299273e+02],\n",
    "        [0.00000000e+00, 1.06969222e+03, 9.44910636e+02],\n",
    "        [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]\n",
    "    ])\n",
    "\n",
    "    fx = camera_intrinsics[0, 0]\n",
    "    fy = camera_intrinsics[1, 1]\n",
    "    cx = camera_intrinsics[0, 2]\n",
    "    cy = camera_intrinsics[1, 2]\n",
    "\n",
    "    # Create meshgrid of pixel coordinates\n",
    "    x, y = np.meshgrid(np.arange(width), np.arange(height))\n",
    "    \n",
    "    # Back-project to 3D\n",
    "    x_3d = (x - cx) * depth_map / fx\n",
    "    y_3d = (y - cy) * depth_map / fy\n",
    "    z_3d = depth_map\n",
    "\n",
    "    # Stack and reshape to points\n",
    "    points = np.stack([x_3d, y_3d, z_3d], axis=-1).reshape(-1, 3)\n",
    "    colors = rgb_image.reshape(-1, 3) / 255.0\n",
    "\n",
    "    # Create point cloud\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "    \n",
    "    # Remove invalid points and statistical outliers\n",
    "    pcd = pcd.voxel_down_sample(voxel_size=0.01)\n",
    "    pcd, _ = pcd.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)\n",
    "    \n",
    "    return pcd\n",
    "\n",
    "def visualize_results(image, depth_map, point_cloud):\n",
    "    \"\"\"Visualize the original image, depth map, and point cloud\"\"\"\n",
    "    # Create subplot\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Original image\n",
    "    ax = fig.add_subplot(131)\n",
    "    ax.imshow(image)\n",
    "    ax.set_title('Original Image')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Depth map\n",
    "    ax = fig.add_subplot(132)\n",
    "    depth_vis = ax.imshow(depth_map, cmap='plasma')\n",
    "    ax.set_title('Depth Map')\n",
    "    ax.axis('off')\n",
    "    plt.colorbar(depth_vis, ax=ax, label='Depth (meters)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize point cloud\n",
    "    coord_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.3)\n",
    "    o3d.visualization.draw_geometries([point_cloud, coord_frame],\n",
    "                                    window_name=\"Metric Point Cloud\",\n",
    "                                    width=1024,\n",
    "                                    height=768)\n",
    "\n",
    "def process_batch(image_paths, output_dir, model, processor, device, camera_matrix, dist_coeffs, batch_size=4):\n",
    "    \"\"\"Process multiple images in batches\"\"\"\n",
    "    import os\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for i in tqdm(range(0, len(image_paths), batch_size)):\n",
    "        batch_paths = image_paths[i:i + batch_size]\n",
    "        for image_path in batch_paths:\n",
    "            try:\n",
    "                # Process single image\n",
    "                image, depth_map = estimate_depth(image_path, model, processor, device, camera_matrix, dist_coeffs)\n",
    "                point_cloud = create_point_cloud(image, depth_map)\n",
    "                \n",
    "                # Save point cloud\n",
    "                output_path = os.path.join(output_dir, \n",
    "                    os.path.splitext(os.path.basename(image_path))[0] + \".ply\")\n",
    "                o3d.io.write_point_cloud(output_path, point_cloud)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {image_path}: {e}\")\n",
    "\n",
    "def main():\n",
    "    # Parameters\n",
    "    image_path = \"checkerboards/left eye checkerboard 5x7 B4/PHOTO_0.jpg\"\n",
    "    output_path = \"output_pointcloud.ply\"\n",
    "    camera_matrix = np.array([\n",
    "        [1.06936178e+03, 0.00000000e+00, 5.55299273e+02],\n",
    "        [0.00000000e+00, 1.06969222e+03, 9.44910636e+02],\n",
    "        [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]\n",
    "    ])\n",
    "    dist_coeffs = np.array([-1.99776871e+01,  1.00589620e+02, -2.71777936e-04, -2.43811396e-03,\n",
    "    4.17712267e+00, -1.96040869e+01,  9.31088031e+01,  4.20762430e+01])\n",
    "    \n",
    "    \n",
    "    # Load model with MPS acceleration\n",
    "    print(\"Loading Depth Anything V2 model...\")\n",
    "    model, processor, device = load_depth_model()\n",
    "\n",
    "    # Process image\n",
    "    print(\"Estimating depth...\")\n",
    "    image, depth_map = estimate_depth(image_path, model, processor, device, camera_matrix, dist_coeffs)\n",
    "    \n",
    "    # Create point cloud\n",
    "    print(\"Generating point cloud...\")\n",
    "    point_cloud = create_point_cloud(image, depth_map)\n",
    "    \n",
    "    # Save point cloud\n",
    "    o3d.io.write_point_cloud(output_path, point_cloud)\n",
    "    print(f\"Point cloud saved to {output_path}\")\n",
    "    \n",
    "    # Visualize results\n",
    "    print(\"Displaying results...\")\n",
    "    visualize_results(image, depth_map, point_cloud)\n",
    "    \n",
    "    # Print statistics\n",
    "    points = np.asarray(point_cloud.points)\n",
    "    print(\"\\nPoint Cloud Statistics:\")\n",
    "    print(f\"Number of points: {len(points)}\")\n",
    "    print(f\"X range: {points[:,0].min():.2f} to {points[:,0].max():.2f} meters\")\n",
    "    print(f\"Y range: {points[:,1].min():.2f} to {points[:,1].max():.2f} meters\")\n",
    "    print(f\"Z range: {points[:,2].min():.2f} to {points[:,2].max():.2f} meters\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # For processing a single image\n",
    "    main()\n",
    "    \n",
    "    # For processing multiple images\n",
    "    # image_paths = glob.glob(\"path/to/images/*.jpg\")\n",
    "    # output_dir = \"path/to/output\"\n",
    "    # model, processor, device = load_depth_model()\n",
    "    # process_batch(image_paths, output_dir, model, processor, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating visualization video...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating video:   0%|          | 0/429 [00:00<?, ?it/s]2025-04-08 11:09:17.233 python[29509:6827572] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-04-08 11:09:17.233 python[29509:6827572] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n",
      "Creating video: 100%|██████████| 429/429 [00:10<00:00, 41.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete!\n",
      "Point clouds saved in: point_clouds\n",
      "Visualization video saved as: point_cloud_visualization.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, AutoModelForDepthEstimation\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Camera intrinsics\n",
    "K_meters = np.array([\n",
    "    [1.06933425e+03, 0.00000000e+00, 5.55076352e+02],\n",
    "    [0.00000000e+00, 1.06959363e+03, 9.45074567e+02],\n",
    "    [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]\n",
    "])\n",
    "\n",
    "def get_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "def load_depth_model(model_name=\"depth-anything/Depth-Anything-V2-Metric-Indoor-Large-hf\"):\n",
    "    device = get_device()\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    image_processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "    model = AutoModelForDepthEstimation.from_pretrained(model_name)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model, image_processor, device\n",
    "\n",
    "def estimate_depth(frame, model, processor, device):\n",
    "    \"\"\"Estimate depth from video frame\"\"\"\n",
    "    # Convert frame to PIL Image\n",
    "    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move inputs to device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predicted_depth = outputs.predicted_depth\n",
    "\n",
    "    prediction = torch.nn.functional.interpolate(\n",
    "        predicted_depth.unsqueeze(1),\n",
    "        size=image.size[::-1],\n",
    "        mode=\"bicubic\",\n",
    "        align_corners=False,\n",
    "    ).squeeze()\n",
    "\n",
    "    return image, prediction.cpu().numpy()\n",
    "\n",
    "def create_point_cloud(rgb_image, depth_map, camera_intrinsics):\n",
    "    \"\"\"Convert RGB-D to point cloud using camera intrinsics\"\"\"\n",
    "    rgb = np.array(rgb_image)\n",
    "    height, width = depth_map.shape\n",
    "    \n",
    "    fx = camera_intrinsics[0, 0]\n",
    "    fy = camera_intrinsics[1, 1]\n",
    "    cx = camera_intrinsics[0, 2]\n",
    "    cy = camera_intrinsics[1, 2]\n",
    "\n",
    "    # Create meshgrid of pixel coordinates\n",
    "    x, y = np.meshgrid(np.arange(width), np.arange(height))\n",
    "    \n",
    "    # Back-project to 3D\n",
    "    x_3d = (x - cx) * depth_map / fx\n",
    "    y_3d = (y - cy) * depth_map / fy\n",
    "    z_3d = depth_map\n",
    "\n",
    "    # Stack and reshape to points\n",
    "    points = np.stack([x_3d, y_3d, z_3d], axis=-1).reshape(-1, 3)\n",
    "    colors = rgb.reshape(-1, 3) / 255.0\n",
    "\n",
    "    # Create point cloud\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "    \n",
    "    # Downsample and remove outliers\n",
    "    pcd = pcd.voxel_down_sample(voxel_size=0.02)\n",
    "    pcd, _ = pcd.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)\n",
    "    \n",
    "    return pcd\n",
    "\n",
    "def process_video(video_path, output_dir):\n",
    "    \"\"\"Process video frames and generate point clouds\"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load model\n",
    "    model, processor, device = load_depth_model()\n",
    "    \n",
    "    # Open video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Process frames\n",
    "    for frame_idx in tqdm(range(total_frames), desc=\"Processing frames\"):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # Process frame\n",
    "        image, depth_map = estimate_depth(frame, model, processor, device)\n",
    "        point_cloud = create_point_cloud(image, depth_map, K_meters)\n",
    "        \n",
    "        # Save point cloud\n",
    "        output_path = os.path.join(output_dir, f\"frame_{frame_idx:05d}.ply\")\n",
    "        o3d.io.write_point_cloud(output_path, point_cloud)\n",
    "    \n",
    "    cap.release()\n",
    "\n",
    "def create_visualization_video(point_clouds_dir, output_video_path, fps=30):\n",
    "    \"\"\"Create a video of rotating point clouds from camera perspective\"\"\"\n",
    "    pcd_files = sorted(glob.glob(os.path.join(point_clouds_dir, \"*.ply\")))\n",
    "    \n",
    "    if not pcd_files:\n",
    "        print(\"No point cloud files found!\")\n",
    "        return\n",
    "        \n",
    "    # Create visualizer\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window(width=1000, height=500)\n",
    "    \n",
    "    # Set up render options\n",
    "    opt = vis.get_render_option()\n",
    "    opt.background_color = np.asarray([0, 0, 0])\n",
    "    opt.point_size = 4.0\n",
    "    \n",
    "    # Load first point cloud to set up view\n",
    "    first_pcd = o3d.io.read_point_cloud(pcd_files[0])\n",
    "    \n",
    "    # Fix orientation: rotate 180 degrees around X-axis\n",
    "    R = first_pcd.get_rotation_matrix_from_xyz((np.pi, 0, 0))\n",
    "    first_pcd.rotate(R, center=(0, 0, 0))\n",
    "    \n",
    "    vis.add_geometry(first_pcd)\n",
    "    \n",
    "    # Set up camera view parameters\n",
    "    ctr = vis.get_view_control()\n",
    "    \n",
    "    # Get camera parameters\n",
    "    cam = ctr.convert_to_pinhole_camera_parameters()\n",
    "    \n",
    "    # Set up front view (camera perspective)\n",
    "    # Assuming Z is forward, Y is up, X is right\n",
    "    cam.extrinsic = np.array([\n",
    "        [1, 0, 0, 0],\n",
    "        [0, -1, 0, 0],\n",
    "        [0, 0, -1, 2],  # Adjust the last value (2) to move camera back/forward\n",
    "        [0, 0, 0, 1]\n",
    "    ])\n",
    "    \n",
    "    ctr.convert_from_pinhole_camera_parameters(cam)\n",
    "    \n",
    "    # Adjust zoom level\n",
    "    ctr.set_zoom(0.1)  # Adjust this value to change zoom level (smaller = more zoom)\n",
    "    \n",
    "    # Create video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video_writer = cv2.VideoWriter(output_video_path, fourcc, fps, (1920, 1080))\n",
    "    \n",
    "    # Process each frame\n",
    "    rotation_angle = 360.0 / len(pcd_files)  # Complete rotation over all frames\n",
    "    \n",
    "    for i, pcd_file in enumerate(tqdm(pcd_files, desc=\"Creating video\")):\n",
    "        # Load and process point cloud\n",
    "        pcd = o3d.io.read_point_cloud(pcd_file)\n",
    "        \n",
    "        # Apply same orientation fix\n",
    "        pcd.rotate(R, center=(0, 0, 0))\n",
    "        \n",
    "        # Update visualization\n",
    "        vis.clear_geometries()\n",
    "        vis.add_geometry(pcd)\n",
    "        \n",
    "        # Optional: Add subtle rotation for visualization\n",
    "        # Comment out these lines if you don't want any rotation\n",
    "        # if i > 0:\n",
    "        #     ctr.rotate(rotation_angle * 0.1, 0.0)  # Reduced rotation speed\n",
    "        \n",
    "        # Capture frame\n",
    "        vis.poll_events()\n",
    "        vis.update_renderer()\n",
    "        image = vis.capture_screen_float_buffer(False)\n",
    "        video_frame = np.asarray(image) * 255\n",
    "        video_frame = cv2.cvtColor(video_frame.astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
    "        video_writer.write(video_frame)\n",
    "    \n",
    "    # Clean up\n",
    "    vis.destroy_window()\n",
    "    video_writer.release()\n",
    "def main():\n",
    "    video_path = \"videos/mono videos/cashier_short (1).mp4\"\n",
    "    output_dir = \"point_clouds\"\n",
    "    output_video = \"point_cloud_visualization.mp4\"\n",
    "    \n",
    "    # # Process video frames to point clouds\n",
    "    # print(\"Processing video frames...\")\n",
    "    # process_video(video_path, output_dir)\n",
    "    \n",
    "    # Create visualization video\n",
    "    print(\"Creating visualization video...\")\n",
    "    create_visualization_video(output_dir, output_video)\n",
    "    \n",
    "    print(\"Processing complete!\")\n",
    "    print(f\"Point clouds saved in: {output_dir}\")\n",
    "    print(f\"Visualization video saved as: {output_video}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Point cloud from video with person detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, AutoModelForDepthEstimation\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Camera intrinsics\n",
    "K_meters = np.array([\n",
    "    [1.06933425e+03, 0.00000000e+00, 5.55076352e+02],\n",
    "    [0.00000000e+00, 1.06959363e+03, 9.45074567e+02],\n",
    "    [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]\n",
    "])\n",
    "\n",
    "def load_yolo():\n",
    "    \"\"\"Load YOLOv8 model for person detection\"\"\"\n",
    "    model = YOLO('yolov8n.pt')  # Load the smallest YOLOv8 model\n",
    "    return model\n",
    "\n",
    "def detect_persons(frame, yolo_model):\n",
    "    \"\"\"Detect persons in frame using YOLOv8\"\"\"\n",
    "    results = yolo_model(frame, classes=[0])  # class 0 is person in COCO\n",
    "    boxes = []\n",
    "    \n",
    "    for result in results:\n",
    "        boxes_data = result.boxes\n",
    "        for box in boxes_data:\n",
    "            if box.conf > 0.5:  # Confidence threshold\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                boxes.append((x1, y1, x2, y2))\n",
    "    \n",
    "    return boxes\n",
    "\n",
    "def crop_and_pad_box(frame, box, padding=50):\n",
    "    \"\"\"Crop frame to box with padding and maintain aspect ratio\"\"\"\n",
    "    x1, y1, x2, y2 = box\n",
    "    h, w = frame.shape[:2]\n",
    "    \n",
    "    # Add padding\n",
    "    x1 = max(0, x1 - padding)\n",
    "    y1 = max(0, y1 - padding)\n",
    "    x2 = min(w, x2 + padding)\n",
    "    y2 = min(h, y2 + padding)\n",
    "    \n",
    "    return frame[y1:y2, x1:x2], (x1, y1, x2, y2)\n",
    "def create_point_cloud(rgb_image, depth_map, camera_intrinsics):\n",
    "    \"\"\"Convert RGB-D to point cloud using camera intrinsics\"\"\"\n",
    "    # Convert PIL Image to numpy array if needed\n",
    "    if isinstance(rgb_image, Image.Image):\n",
    "        rgb = np.array(rgb_image)\n",
    "    else:\n",
    "        rgb = rgb_image\n",
    "        \n",
    "    height, width = depth_map.shape\n",
    "    \n",
    "    fx = camera_intrinsics[0, 0]\n",
    "    fy = camera_intrinsics[1, 1]\n",
    "    cx = camera_intrinsics[0, 2]\n",
    "    cy = camera_intrinsics[1, 2]\n",
    "\n",
    "    # Create meshgrid of pixel coordinates\n",
    "    x, y = np.meshgrid(np.arange(width), np.arange(height))\n",
    "    \n",
    "    # Back-project to 3D\n",
    "    x_3d = (x - cx) * depth_map / fx\n",
    "    y_3d = (y - cy) * depth_map / fy\n",
    "    z_3d = depth_map\n",
    "\n",
    "    # Stack and reshape to points\n",
    "    points = np.stack([x_3d, y_3d, z_3d], axis=-1).reshape(-1, 3)\n",
    "    colors = rgb.reshape(-1, 3) / 255.0\n",
    "\n",
    "    # Create point cloud\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "    \n",
    "    # Downsample and remove outliers\n",
    "    pcd = pcd.voxel_down_sample(voxel_size=0.02)\n",
    "    pcd, _ = pcd.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)\n",
    "    \n",
    "    return pcd\n",
    "\n",
    "def estimate_depth(frame, model, processor, device):\n",
    "    \"\"\"Estimate depth from video frame\"\"\"\n",
    "    # Convert frame to PIL Image if it's a numpy array\n",
    "    if isinstance(frame, np.ndarray):\n",
    "        # Convert BGR to RGB if needed\n",
    "        if frame.shape[2] == 3:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image = Image.fromarray(frame)\n",
    "    else:\n",
    "        image = frame\n",
    "    \n",
    "    # Process image\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predicted_depth = outputs.predicted_depth\n",
    "\n",
    "    prediction = torch.nn.functional.interpolate(\n",
    "        predicted_depth.unsqueeze(1),\n",
    "        size=image.size[::-1],\n",
    "        mode=\"bicubic\",\n",
    "        align_corners=False,\n",
    "    ).squeeze()\n",
    "\n",
    "    return np.array(image), prediction.cpu().numpy()\n",
    "\n",
    "def process_video_with_person_detection(video_path, output_dir):\n",
    "    \"\"\"Process video frames with person detection and generate point clouds\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load models\n",
    "    yolo_model = load_yolo()\n",
    "    depth_model, processor, device = load_depth_model()\n",
    "    \n",
    "    # Open video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    for frame_idx in tqdm(range(total_frames), desc=\"Processing frames\"):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Detect persons\n",
    "        person_boxes = detect_persons(frame, yolo_model)\n",
    "        \n",
    "        if not person_boxes:\n",
    "            continue\n",
    "        \n",
    "        # Process each person detection\n",
    "        for box_idx, box in enumerate(person_boxes):\n",
    "            # Crop frame to person with padding\n",
    "            cropped_frame, actual_box = crop_and_pad_box(frame, box)\n",
    "            \n",
    "            # Skip if crop is too small\n",
    "            if cropped_frame.shape[0] < 64 or cropped_frame.shape[1] < 64:\n",
    "                continue\n",
    "            \n",
    "            # Process cropped frame\n",
    "            image, depth_map = estimate_depth(cropped_frame, depth_model, processor, device)\n",
    "            \n",
    "            # Adjust camera intrinsics for crop\n",
    "            x1, y1, _, _ = actual_box\n",
    "            adjusted_K = K_meters.copy()\n",
    "            adjusted_K[0, 2] = K_meters[0, 2] - x1  # Adjust cx\n",
    "            adjusted_K[1, 2] = K_meters[1, 2] - y1  # Adjust cy\n",
    "            \n",
    "            # Create point cloud\n",
    "            point_cloud = create_point_cloud(image, depth_map, adjusted_K)\n",
    "            \n",
    "            # Save point cloud\n",
    "            output_path = os.path.join(output_dir, f\"frame_{frame_idx:05d}_person_{box_idx:02d}.ply\")\n",
    "            o3d.io.write_point_cloud(output_path, point_cloud)\n",
    "    \n",
    "    cap.release()\n",
    "\n",
    "def visualize_detection_and_depth(frame, box, depth_map):\n",
    "    \"\"\"Visualize person detection and corresponding depth map\"\"\"\n",
    "    x1, y1, x2, y2 = box\n",
    "    \n",
    "    # Draw detection box\n",
    "    frame_viz = frame.copy()\n",
    "    cv2.rectangle(frame_viz, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    \n",
    "    # Normalize depth map for visualization\n",
    "    depth_viz = ((depth_map - depth_map.min()) / (depth_map.max() - depth_map.min()) * 255).astype(np.uint8)\n",
    "    depth_viz = cv2.applyColorMap(depth_viz, cv2.COLORMAP_PLASMA)\n",
    "    \n",
    "    # Combine visualizations\n",
    "    return np.hstack((frame_viz, depth_viz))\n",
    "\n",
    "def create_visualization_video_with_detection(point_clouds_dir, output_video_path, fps=30):\n",
    "    \"\"\"Create video showing original frame, person detection, and point cloud\"\"\"\n",
    "    pcd_files = sorted(glob.glob(os.path.join(point_clouds_dir, \"*.ply\")))\n",
    "    \n",
    "    if not pcd_files:\n",
    "        print(\"No point cloud files found!\")\n",
    "        return\n",
    "    \n",
    "    # Create visualizer\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window(width=800, height=600)\n",
    "    \n",
    "    # Set up render options\n",
    "    opt = vis.get_render_option()\n",
    "    opt.background_color = np.asarray([0, 0, 0])\n",
    "    opt.point_size = 4.0\n",
    "    \n",
    "    # Video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video_writer = cv2.VideoWriter(output_video_path, fourcc, fps, (1600, 600))\n",
    "    \n",
    "    for pcd_file in tqdm(pcd_files, desc=\"Creating video\"):\n",
    "        # Load point cloud\n",
    "        pcd = o3d.io.read_point_cloud(pcd_file)\n",
    "        \n",
    "        # Fix orientation\n",
    "        R = pcd.get_rotation_matrix_from_xyz((np.pi, 0, 0))\n",
    "        pcd.rotate(R, center=(0, 0, 0))\n",
    "        \n",
    "        # Update visualization\n",
    "        vis.clear_geometries()\n",
    "        vis.add_geometry(pcd)\n",
    "        \n",
    "        # Capture frame\n",
    "        vis.poll_events()\n",
    "        vis.update_renderer()\n",
    "        pcd_image = vis.capture_screen_float_buffer(False)\n",
    "        pcd_frame = (np.asarray(pcd_image) * 255).astype(np.uint8)\n",
    "        \n",
    "        # Write frame\n",
    "        video_writer.write(pcd_frame)\n",
    "\n",
    "    \n",
    "    # Clean up\n",
    "    vis.destroy_window()\n",
    "\n",
    "    video_writer.release()\n",
    "\n",
    "def main():\n",
    "    video_path = \"left eye videos/Video_1_L.mp4\"\n",
    "    output_dir = \"person_point_clouds\"\n",
    "    output_video = \"person_point_cloud_visualization.mp4\"\n",
    "    \n",
    "    # # Process video with person detection\n",
    "    # print(\"Processing video frames...\")\n",
    "    # process_video_with_person_detection(video_path, output_dir)\n",
    "    \n",
    "    # Create visualization video\n",
    "    print(\"Creating visualization video...\")\n",
    "    create_visualization_video_with_detection(output_dir, output_video)\n",
    "    \n",
    "    print(\"Processing complete!\")\n",
    "    print(f\"Point clouds saved in: {output_dir}\")\n",
    "    print(f\"Visualization video saved as: {output_video}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video annotation with Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create grid images from video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Grid size exceeds final image size. Cropping grid.\n",
      "Grid image saved to: output_grid.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "\n",
    "def create_video_grid(video_path, output_image_path, target_frame_size=(252*2, 448*2), grid_shape=(1, 3), frame_skip=10, final_grid_size=(1344, 1344)):\n",
    "    \"\"\"\n",
    "    Opens a video, selects frames, downscales them, arranges them in a grid,\n",
    "    and saves the final grid image (padded if necessary).\n",
    "\n",
    "    Args:\n",
    "        video_path (str): Path to the input MP4 video file.\n",
    "        output_image_path (str): Path to save the output grid image (e.g., 'grid.jpg').\n",
    "        target_frame_size (tuple): Desired (width, height) for each frame in the grid.\n",
    "        grid_shape (tuple): Desired (rows, cols) for the grid.\n",
    "        frame_skip (int): Number of frames to skip between selected frames.\n",
    "        final_grid_size (tuple): Final desired (width, height) for the output image.\n",
    "    \"\"\"\n",
    "    target_w, target_h = target_frame_size\n",
    "    grid_rows, grid_cols = grid_shape\n",
    "    final_w, final_h = final_grid_size\n",
    "    num_frames_needed = grid_rows * grid_cols\n",
    "\n",
    "    # --- 1. Open Video ---\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file: {video_path}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Select and Downscale Frames ---\n",
    "    selected_frames = []\n",
    "    frame_count = 0\n",
    "    selected_count = 0\n",
    "\n",
    "    while selected_count < num_frames_needed:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Warning: Reached end of video before collecting enough frames.\")\n",
    "            break\n",
    "\n",
    "        # Select frame based on index (0, 11, 22, ...)\n",
    "        if frame_count % (frame_skip + 1) == 0:\n",
    "            # Downscale the frame\n",
    "            downscaled_frame = cv2.resize(frame, target_frame_size, interpolation=cv2.INTER_AREA)\n",
    "            selected_frames.append(downscaled_frame)\n",
    "            selected_count += 1\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    if len(selected_frames) != num_frames_needed:\n",
    "        print(f\"Error: Collected {len(selected_frames)} frames, but needed {num_frames_needed}.\")\n",
    "        return\n",
    "\n",
    "    # --- 3. Arrange Frames into Grid ---\n",
    "    grid_h = target_h * grid_rows\n",
    "    grid_w = target_w * grid_cols\n",
    "    grid_image = np.zeros((grid_h, grid_w, 3), dtype=np.uint8)\n",
    "\n",
    "    for i, frame in enumerate(selected_frames):\n",
    "        row = i // grid_cols\n",
    "        col = i % grid_cols\n",
    "        y_offset = row * target_h\n",
    "        x_offset = col * target_w\n",
    "        grid_image[y_offset:y_offset + target_h, x_offset:x_offset + target_w] = frame\n",
    "\n",
    "    # --- 4. Pad Grid to Final Size ---\n",
    "    # Create a black background\n",
    "    final_image = np.zeros((final_h, final_w, 3), dtype=np.uint8)\n",
    "\n",
    "    # Calculate offsets to center the grid\n",
    "    pad_top = (final_h - grid_h) // 2\n",
    "    pad_left = (final_w - grid_w) // 2\n",
    "\n",
    "    # Ensure padding is non-negative (grid isn't larger than final size)\n",
    "    pad_top = max(0, pad_top)\n",
    "    pad_left = max(0, pad_left)\n",
    "\n",
    "    # Calculate the region to place the grid\n",
    "    end_row = pad_top + grid_h\n",
    "    end_col = pad_left + grid_w\n",
    "\n",
    "    # Check bounds to prevent errors if grid is larger than final image (shouldn't happen with padding calc)\n",
    "    if end_row <= final_h and end_col <= final_w:\n",
    "         final_image[pad_top:end_row, pad_left:end_col] = grid_image\n",
    "    else:\n",
    "         # Handle cases where the grid might be larger (e.g., crop the grid)\n",
    "         print(\"Warning: Grid size exceeds final image size. Cropping grid.\")\n",
    "         crop_h = min(grid_h, final_h - pad_top)\n",
    "         crop_w = min(grid_w, final_w - pad_left)\n",
    "         final_image[pad_top:pad_top + crop_h, pad_left:pad_left + crop_w] = grid_image[:crop_h, :crop_w]\n",
    "\n",
    "\n",
    "    # --- 5. Save the Final Image ---\n",
    "    cv2.imwrite(output_image_path, final_image)\n",
    "    print(f\"Grid image saved to: {output_image_path}\")\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Make sure the video file exists at this path\n",
    "input_video = 'videos/mono videos/cashier_short (1).mp4'\n",
    "output_image = 'output_grid.jpg'\n",
    "\n",
    "# Create an output directory if it doesn't exist\n",
    "output_dir = os.path.dirname(output_image)\n",
    "if output_dir and not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "if os.path.exists(input_video):\n",
    "    create_video_grid(input_video, output_image)\n",
    "else:\n",
    "    print(f\"Error: Input video not found at {input_video}\")\n",
    "    # As a fallback, create a dummy black image if video is missing\n",
    "    print(\"Creating a dummy black image instead.\")\n",
    "    dummy_image = np.zeros((1344, 1344, 3), dtype=np.uint8)\n",
    "    cv2.imwrite(output_image, dummy_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimized 3x3 grid for CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid image saved to: output_grid.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "\n",
    "def create_video_grid(video_path, output_image_path, target_frame_size=(448, 448), grid_shape=(3, 3), frame_skip=10, final_grid_size=(1344, 1344)):\n",
    "    \"\"\"\n",
    "    Opens a video, selects frames, downscales them, arranges them in a 3x3 grid,\n",
    "    and saves the final 1344x1344 grid image.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): Path to the input MP4 video file.\n",
    "        output_image_path (str): Path to save the output grid image (e.g., 'grid.jpg').\n",
    "        target_frame_size (tuple): Desired (width, height) for each frame in the grid (should be 448x448 for a 3x3 in 1344x1344).\n",
    "        grid_shape (tuple): Desired (rows, cols) for the grid (should be 3x3).\n",
    "        frame_skip (int): Number of frames to skip between selected frames.\n",
    "        final_grid_size (tuple): Final desired (width, height) for the output image (should be 1344x1344).\n",
    "    \"\"\"\n",
    "    target_w, target_h = target_frame_size\n",
    "    grid_rows, grid_cols = grid_shape\n",
    "    final_w, final_h = final_grid_size\n",
    "    num_frames_needed = grid_rows * grid_cols # This will be 9 for a 3x3 grid\n",
    "\n",
    "    # --- Sanity check for optimal size ---\n",
    "    if grid_rows * target_h != final_h or grid_cols * target_w != final_w:\n",
    "        print(f\"Warning: Grid dimensions ({grid_cols*target_w}x{grid_rows*target_h}) \"\n",
    "              f\"do not perfectly match final size ({final_w}x{final_h}). Padding or cropping might occur if mismatch.\")\n",
    "    if grid_rows != 3 or grid_cols != 3:\n",
    "         print(f\"Warning: grid_shape is set to {grid_shape}, not the optimal 3x3 for this request.\")\n",
    "    if target_w != 448 or target_h != 448:\n",
    "         print(f\"Warning: target_frame_size is set to {target_frame_size}, not the optimal 448x448 for a 3x3 grid in a 1344x1344 image.\")\n",
    "\n",
    "\n",
    "    # --- 1. Open Video ---\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file: {video_path}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Select and Downscale Frames ---\n",
    "    selected_frames = []\n",
    "    frame_count = 0\n",
    "    selected_count = 0\n",
    "\n",
    "    while selected_count < num_frames_needed:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Warning: Reached end of video before collecting enough frames.\")\n",
    "            break\n",
    "\n",
    "        # Select frame based on index (0, 11, 22, ...)\n",
    "        if frame_count % (frame_skip + 1) == 0:\n",
    "            # Downscale the frame to target_frame_size (448x448)\n",
    "            downscaled_frame = cv2.resize(frame, target_frame_size, interpolation=cv2.INTER_AREA)\n",
    "            selected_frames.append(downscaled_frame)\n",
    "            selected_count += 1\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    if len(selected_frames) != num_frames_needed:\n",
    "        print(f\"Error: Collected {len(selected_frames)} frames, but needed {num_frames_needed}.\")\n",
    "        return\n",
    "\n",
    "    # --- 3. Arrange Frames into Grid ---\n",
    "    # Calculate the grid dimensions based on cell size and shape (will be 1344x1344)\n",
    "    grid_h = target_h * grid_rows\n",
    "    grid_w = target_w * grid_cols\n",
    "    grid_image = np.zeros((grid_h, grid_w, 3), dtype=np.uint8)\n",
    "\n",
    "    for i, frame in enumerate(selected_frames):\n",
    "        row = i // grid_cols\n",
    "        col = i % grid_cols\n",
    "        y_offset = row * target_h\n",
    "        x_offset = col * target_w\n",
    "        # Place the 448x448 frame into the correct cell\n",
    "        grid_image[y_offset:y_offset + target_h, x_offset:x_offset + target_w] = frame\n",
    "\n",
    "    # --- 4. Pad Grid to Final Size (Padding should be zero in this optimal case) ---\n",
    "    # Create a black background of the final size (1344x1344)\n",
    "    final_image = np.zeros((final_h, final_w, 3), dtype=np.uint8)\n",
    "\n",
    "    # Calculate offsets to center the grid (should be 0)\n",
    "    pad_top = (final_h - grid_h) // 2\n",
    "    pad_left = (final_w - grid_w) // 2\n",
    "\n",
    "    # Ensure padding is non-negative (still good practice)\n",
    "    pad_top = max(0, pad_top)\n",
    "    pad_left = max(0, pad_left)\n",
    "\n",
    "    # Calculate the region to place the grid (end_row/col should be 1344)\n",
    "    end_row = pad_top + grid_h\n",
    "    end_col = pad_left + grid_w\n",
    "\n",
    "    # Place the constructed grid_image (1344x1344) onto the final_image (1344x1344)\n",
    "    # The bounds check simplifies as the sizes match\n",
    "    if grid_h <= final_h and grid_w <= final_w:\n",
    "         final_image[pad_top:end_row, pad_left:end_col] = grid_image\n",
    "    else:\n",
    "         # This case shouldn't happen with 448x448 cells in a 1344x1344 grid\n",
    "         print(\"Warning: Grid size exceeds final image size unexpectedly. Cropping.\")\n",
    "         crop_h = min(grid_h, final_h - pad_top)\n",
    "         crop_w = min(grid_w, final_w - pad_left)\n",
    "         final_image[pad_top:pad_top + crop_h, pad_left:pad_left + crop_w] = grid_image[:crop_h, :crop_w]\n",
    "\n",
    "    # --- 5. Save the Final Image ---\n",
    "    cv2.imwrite(output_image_path, final_image)\n",
    "    print(f\"Grid image saved to: {output_image_path}\")\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Make sure the video file exists at this path\n",
    "input_video = 'videos/mono videos/cashier_short (1).mp4'\n",
    "output_image = 'output_grid.jpg'\n",
    "\n",
    "# Create an output directory if it doesn't exist\n",
    "output_dir = os.path.dirname(output_image)\n",
    "if output_dir and not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "if os.path.exists(input_video):\n",
    "    # Call the function with defaults (which are now optimal for 3x3 in 1344x1344)\n",
    "    create_video_grid(input_video, output_image)\n",
    "    # Or explicitly specify:\n",
    "    # create_video_grid(input_video, output_image, target_frame_size=(448, 448), grid_shape=(3, 3), final_grid_size=(1344, 1344))\n",
    "else:\n",
    "    print(f\"Error: Input video not found at {input_video}\")\n",
    "    # As a fallback, create a dummy black image if video is missing\n",
    "    print(\"Creating a dummy black image instead.\")\n",
    "    dummy_image = np.zeros((1344, 1344, 3), dtype=np.uint8)\n",
    "    cv2.imwrite(output_image, dummy_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annotate single 1344x1344 image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading image: output_grid.jpg\n",
      "Using model: minicpm-v:8b-2.6-fp16\n",
      "Sending request to Ollama...\n",
      "The cashier places items from a shopping cart onto a conveyor belt.\n",
      "Completed streaming response from Ollama.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "\n",
    "#models:\n",
    "# minicpm-v:8b-2.6-fp16 \n",
    "# minicpm-v:8b-2.6-q4_K_M\n",
    "def get_image_description(image_path, model_name=\"minicpm-v:8b-2.6-fp16\"):\n",
    "    \"\"\"\n",
    "    Loads an image, sends it to Ollama with a prompt, and streams the description.\n",
    "\n",
    "    Args:\n",
    "        image_path (str or Path): Path to the image file.\n",
    "        model_name (str): The Ollama model to use (should support vision).\n",
    "\n",
    "    Returns:\n",
    "        str: The description generated by the model, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    image_path = Path(image_path)\n",
    "    if not image_path.is_file():\n",
    "        print(f\"Error: Image file not found at {image_path}\")\n",
    "        return None\n",
    "\n",
    "    # Updated few-shot prompt\n",
    "    prompt = \"\"\"\n",
    "# --- Task Definition ---\n",
    "You are an expert AI analyzing video frames from a first-person camera worn by a checkout cashier who remains primarily stationary.\n",
    "Your goal is to identify the **single most prominent or representative action** performed by the cashier within the provided video frame sequence.\n",
    "\n",
    "**Instructions for Output:**\n",
    "1.  Output **only one single phrase**.\n",
    "2.  The phrase must start with \"The cashier...\".\n",
    "3.  Describe the single chosen action concisely (e.g., reaching, grabbing, placing, scanning, tapping, looking, waiting).\n",
    "4.  Mention the hand used (e.g., \"with their right hand\", \"with their left hand\") if the action involves a specific hand.\n",
    "5.  Include key object and location details relevant to that single action.\n",
    "\n",
    "# --- Examples of Desired Single Output Phrase ---\n",
    "\n",
    "*Example Single Output Phrase (based on a hypothetical video segment):*\n",
    "The cashier reaches with their right hand for a tray of meat that is laying inside the shopping cart.\n",
    "\n",
    "*Example Single Output Phrase (based on a different hypothetical video segment):*\n",
    "The cashier taps the code 3214 on the payment terminal to their right using the left hand.\n",
    "\n",
    "*Example Single Output Phrase (based on another hypothetical video segment):*\n",
    "The cashier is placing a banana into the bag that is on their right with their left hand.\n",
    "\n",
    "# --- Current Task ---\n",
    "\n",
    "Analyze the following sequence of video frames provided. Identify the single most prominent action and describe it in **one single phrase**:\n",
    "\n",
    "*Output Phrase:*\n",
    "\"\"\"\n",
    "\n",
    "    print(f\"Loading image: {image_path}\")\n",
    "    print(f\"Using model: {model_name}\")\n",
    "    # Print only the start of the long prompt for brevity in logs\n",
    "    # print(f\"Using prompt (start): '{prompt[:100]}...'\")\n",
    "\n",
    "    try:\n",
    "        # Load image and convert to base64\n",
    "        with Image.open(image_path) as img:\n",
    "            # Ensure image is in RGB format\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "\n",
    "            # Save image to a bytes buffer to get bytes data\n",
    "            buffer = io.BytesIO()\n",
    "            img.save(buffer, format=\"JPEG\") # Or PNG\n",
    "            image_bytes = buffer.getvalue()\n",
    "            image_base64 = base64.b64encode(image_bytes).decode('utf-8')\n",
    "\n",
    "        print(\"Sending request to Ollama...\")\n",
    "\n",
    "        # --- Ollama Interaction with Streaming ---\n",
    "        full_response = \"\"\n",
    "        for chunk in ollama.chat(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': prompt,\n",
    "                    'images': [image_base64]\n",
    "                }\n",
    "            ],\n",
    "            stream=True\n",
    "        ):\n",
    "            if 'message' in chunk and 'content' in chunk['message']:\n",
    "                content = chunk['message']['content']\n",
    "                print(content, end='', flush=True)\n",
    "                full_response += content\n",
    "\n",
    "        print(\"\\nCompleted streaming response from Ollama.\")\n",
    "        return full_response\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Image file not found at {image_path}\")\n",
    "    except ImportError:\n",
    "        print(\"Error: The 'ollama' or 'Pillow' library is not installed.\")\n",
    "        print(\"Please install them using: pip install ollama Pillow\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        print(\"Please ensure the Ollama server is running and the model is available.\")\n",
    "    return None\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    grid_image_file = 'output_grid.jpg' # Path to the grid image created previously\n",
    "\n",
    "    description = get_image_description(grid_image_file)\n",
    "\n",
    "    # if description:\n",
    "    #     print(\"\\n--- Complete Description ---\")\n",
    "    #     print(description)\n",
    "    # else:\n",
    "    #     print(\"\\nFailed to get description.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annotate video using N-frames of size 1344x1344"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frames: 502, FPS: 29.96, Initial sampled indices: 17\n",
      "Warning: Could not read frame at index 420\n",
      "Warning: Could not read frame at index 450\n",
      "Warning: Could not read frame at index 480\n",
      "Frames processed for model: 14\n",
      "Processing 14 frames for Ollama.\n",
      "Using model: minicpm-v:8b-2.6-fp16\n",
      "Sending request with 14 base64 frames to Ollama...\n",
      "The cashier grabs an item from the shopping cart to scan at checkout.\n",
      "Completed streaming response from Ollama.\n",
      "\n",
      "--- Final Ollama Description ---\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "import cv2          # Import OpenCV\n",
    "import numpy as np  # Import numpy\n",
    "\n",
    "# --- Video Encoding Function (Using OpenCV) ---\n",
    "# (Keep the same encode_video function using OpenCV as provided in the previous response)\n",
    "def encode_video(video_path, max_num_frames=9):\n",
    "    \"\"\"Samples frames from a video path using OpenCV.\"\"\"\n",
    "    def uniform_sample(l, n):\n",
    "        n = min(n, len(l))\n",
    "        if n == 0: return []\n",
    "        gap = len(l) / n\n",
    "        idxs = [int(i * gap + gap / 2) for i in range(n)]\n",
    "        return [l[min(i, len(l)-1)] for i in idxs]\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file {video_path} with OpenCV.\")\n",
    "        return None\n",
    "\n",
    "    frames_pil = []\n",
    "    try:\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "        if total_frames <= 0 or fps <= 0:\n",
    "             print(f\"Warning: Video file {video_path} has invalid properties (frames={total_frames}, fps={fps}).\")\n",
    "             ret, frame = cap.read()\n",
    "             if ret:\n",
    "                 frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                 frames_pil.append(Image.fromarray(frame_rgb))\n",
    "             return frames_pil\n",
    "\n",
    "        sample_step = max(1, round(fps / 1))\n",
    "        frame_indices_to_sample = [i for i in range(0, total_frames, sample_step)]\n",
    "        print(f\"Total frames: {total_frames}, FPS: {fps:.2f}, Initial sampled indices: {len(frame_indices_to_sample)}\")\n",
    "\n",
    "        if len(frame_indices_to_sample) > max_num_frames:\n",
    "            print(f\"Sampling down to {max_num_frames} frames.\")\n",
    "            frame_indices_to_sample = uniform_sample(frame_indices_to_sample, max_num_frames)\n",
    "\n",
    "        processed_count = 0\n",
    "        for frame_index in frame_indices_to_sample:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames_pil.append(Image.fromarray(frame_rgb))\n",
    "                processed_count += 1\n",
    "            else:\n",
    "                print(f\"Warning: Could not read frame at index {frame_index}\")\n",
    "        print(f'Frames processed for model: {len(frames_pil)}')\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during OpenCV video processing: {e}\")\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return frames_pil\n",
    "\n",
    "# --- Function to get video description using Ollama ---\n",
    "def get_video_description_ollama(video_frames, model_name=\"minicpm-v:8b-2.6-fp16\"):\n",
    "    \"\"\"\n",
    "    Takes a list of PIL video frames, sends them to Ollama with a prompt,\n",
    "    and streams the description.\n",
    "\n",
    "    Args:\n",
    "        video_frames (list): List of PIL Image objects representing video frames.\n",
    "        model_name (str): The Ollama model to use (should support vision).\n",
    "\n",
    "    Returns:\n",
    "        str: The description generated by the model, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    if not video_frames:\n",
    "        print(\"Error: No video frames provided.\")\n",
    "        return None\n",
    "\n",
    "    # Define the prompt for single concise action phrase\n",
    "    prompt = \"\"\"\n",
    "# --- Task Definition ---\n",
    "You are an expert AI analyzing video frames from a first-person camera worn by a checkout cashier who remains primarily stationary.\n",
    "Your goal is to identify the **single most prominent or representative action** performed by the cashier within the provided video frame sequence.\n",
    "\n",
    "**Instructions for Output:**\n",
    "1.  Output **only one single phrase**.\n",
    "2.  The phrase must start with \"The cashier...\".\n",
    "3.  Describe the single chosen action concisely (e.g., reaching, grabbing, placing, scanning, tapping, looking, waiting).\n",
    "4.  Mention the hand used (e.g., \"with their right hand\", \"with their left hand\") if the action involves a specific hand.\n",
    "5.  Include key object and location details relevant to that single action.\n",
    "\n",
    "# --- Examples of Desired Single Output Phrase ---\n",
    "\n",
    "*Example Single Output Phrase (based on a hypothetical video segment):*\n",
    "The cashier reaches with their right hand for a tray of meat that is laying inside the shopping cart.\n",
    "\n",
    "*Example Single Output Phrase (based on a different hypothetical video segment):*\n",
    "The cashier taps the code 3214 on the payment terminal to their right.\n",
    "\n",
    "*Example Single Output Phrase (based on another hypothetical video segment):*\n",
    "The cashier is placing a banana into the bag that is on their right with their left hand.\n",
    "\n",
    "# --- Current Task ---\n",
    "\n",
    "Analyze the following sequence of video frames provided. Identify the single most prominent action and describe it in **one single phrase**:\n",
    "\n",
    "*Output Phrase:*\n",
    "\"\"\"\n",
    "\n",
    "    print(f\"Processing {len(video_frames)} frames for Ollama.\")\n",
    "    print(f\"Using model: {model_name}\")\n",
    "\n",
    "    try:\n",
    "        # Convert each PIL Image frame to base64\n",
    "        images_base64 = []\n",
    "        for frame in video_frames:\n",
    "            buffer = io.BytesIO()\n",
    "            # Ensure frame is RGB before saving\n",
    "            if frame.mode != 'RGB':\n",
    "                frame = frame.convert('RGB')\n",
    "            frame.save(buffer, format=\"JPEG\") # Or PNG\n",
    "            image_bytes = buffer.getvalue()\n",
    "            images_base64.append(base64.b64encode(image_bytes).decode('utf-8'))\n",
    "\n",
    "        print(f\"Sending request with {len(images_base64)} base64 frames to Ollama...\")\n",
    "\n",
    "        # --- Ollama Interaction with Streaming ---\n",
    "        full_response = \"\"\n",
    "        # Make sure Ollama server is running\n",
    "        for chunk in ollama.chat(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': prompt,\n",
    "                    'images': images_base64 # Pass the LIST of base64 encoded frames\n",
    "                }\n",
    "            ],\n",
    "            stream=True\n",
    "        ):\n",
    "            if 'message' in chunk and 'content' in chunk['message']:\n",
    "                content = chunk['message']['content']\n",
    "                print(content, end='', flush=True)\n",
    "                full_response += content\n",
    "\n",
    "        print(\"\\nCompleted streaming response from Ollama.\")\n",
    "        return full_response\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"Error: The 'ollama' or 'Pillow' library is not installed.\")\n",
    "        print(\"Please install them using: pip install ollama Pillow\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during Ollama interaction: {e}\")\n",
    "        print(\"Please ensure the Ollama server is running and the model is available.\")\n",
    "    return None\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    video_path = \"videos/mono videos/cashier_short (1).mp4\" # Path to your video file\n",
    "    max_frames_to_send = 32 # Limit frames sent to Ollama to manage request size/time (adjust as needed)\n",
    "\n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"Error: Video file not found at {video_path}\")\n",
    "    else:\n",
    "        # Encode video to get list of PIL frames\n",
    "        frames = encode_video(video_path, max_num_frames=max_frames_to_send)\n",
    "\n",
    "        if frames:\n",
    "            # Get description using Ollama\n",
    "            description = get_video_description_ollama(frames)\n",
    "\n",
    "            if description:\n",
    "                 print(\"\\n--- Final Ollama Description ---\")\n",
    "                 # The description variable already contains the streamed output\n",
    "                 # You might want to process/clean it further if needed\n",
    "            else:\n",
    "                 print(\"\\nFailed to get description from Ollama.\")\n",
    "        else:\n",
    "            print(\"Could not encode video frames.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annotate video using N-frames of size 448x448"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frames: 502, FPS: 29.96, Initial sampled indices: 17\n",
      "Sampling down to 15 frames.\n",
      "Warning: Could not read frame at index 420\n",
      "Warning: Could not read frame at index 450\n",
      "Warning: Could not read frame at index 480\n",
      "Frames processed for model (resized to (448, 448)): 12\n",
      "Processing 12 frames for Ollama.\n",
      "Using model: minicpm-v:8b-2.6-fp16\n",
      "Sending request with 12 base64 frames to Ollama...\n",
      "The cashier is scanning items into a red shopping cart at Trader Joe's, with various products visible on the conveyor belt including bananas, strawberries, milk, and other groceries.\n",
      "Completed streaming response from Ollama.\n",
      "\n",
      "--- Final Ollama Description ---\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "import cv2          # Import OpenCV\n",
    "import numpy as np  # Import numpy\n",
    "\n",
    "# --- Video Encoding Function (Using OpenCV) ---\n",
    "def encode_video(video_path, target_size=(448, 448), max_num_frames=20):\n",
    "    \"\"\"\n",
    "    Samples frames from a video path using OpenCV and resizes them.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): Path to the input video file.\n",
    "        target_size (tuple): Desired (width, height) for each output frame.\n",
    "        max_num_frames (int): Maximum number of frames to sample and return.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of PIL Image objects, each resized to target_size,\n",
    "              or None if an error occurs.\n",
    "    \"\"\"\n",
    "    def uniform_sample(l, n):\n",
    "        n = min(n, len(l))\n",
    "        if n == 0: return []\n",
    "        gap = len(l) / n\n",
    "        idxs = [int(i * gap + gap / 2) for i in range(n)]\n",
    "        return [l[min(i, len(l)-1)] for i in idxs]\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file {video_path} with OpenCV.\")\n",
    "        return None\n",
    "\n",
    "    frames_pil = []\n",
    "    target_w, target_h = target_size # Unpack target size\n",
    "\n",
    "    try:\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "        if total_frames <= 0 or fps <= 0:\n",
    "             print(f\"Warning: Video file {video_path} has invalid properties (frames={total_frames}, fps={fps}). Trying to read first frame.\")\n",
    "             ret, frame = cap.read()\n",
    "             if ret:\n",
    "                 # Resize the single frame\n",
    "                 resized_frame = cv2.resize(frame, target_size, interpolation=cv2.INTER_AREA)\n",
    "                 frame_rgb = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB)\n",
    "                 frames_pil.append(Image.fromarray(frame_rgb))\n",
    "             return frames_pil\n",
    "\n",
    "        # Sample roughly 1 frame per second\n",
    "        sample_step = max(1, round(fps / 1))\n",
    "        frame_indices_to_sample = [i for i in range(0, total_frames, sample_step)]\n",
    "        print(f\"Total frames: {total_frames}, FPS: {fps:.2f}, Initial sampled indices: {len(frame_indices_to_sample)}\")\n",
    "\n",
    "        # Limit frame count if necessary\n",
    "        if len(frame_indices_to_sample) > max_num_frames:\n",
    "            print(f\"Sampling down to {max_num_frames} frames.\")\n",
    "            frame_indices_to_sample = uniform_sample(frame_indices_to_sample, max_num_frames)\n",
    "\n",
    "        processed_count = 0\n",
    "        # Read, resize, and convert selected frames\n",
    "        for frame_index in frame_indices_to_sample:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                # --- Add Resize Step Here ---\n",
    "                resized_frame = cv2.resize(frame, target_size, interpolation=cv2.INTER_AREA)\n",
    "                # Convert resized frame from BGR to RGB\n",
    "                frame_rgb = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB)\n",
    "                # Convert to PIL Image\n",
    "                frames_pil.append(Image.fromarray(frame_rgb))\n",
    "                processed_count += 1\n",
    "            else:\n",
    "                print(f\"Warning: Could not read frame at index {frame_index}\")\n",
    "        print(f'Frames processed for model (resized to {target_size}): {len(frames_pil)}')\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during OpenCV video processing: {e}\")\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return frames_pil\n",
    "\n",
    "# --- Function to get video description using Ollama ---\n",
    "# ... (get_video_description_ollama function remains the same) ...\n",
    "def get_video_description_ollama(video_frames, model_name=\"minicpm-v:8b-2.6-fp16\"):\n",
    "    \"\"\"\n",
    "    Takes a list of PIL video frames, sends them to Ollama with a prompt,\n",
    "    and streams the description.\n",
    "\n",
    "    Args:\n",
    "        video_frames (list): List of PIL Image objects representing video frames.\n",
    "        model_name (str): The Ollama model to use (should support vision).\n",
    "\n",
    "    Returns:\n",
    "        str: The description generated by the model, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    if not video_frames:\n",
    "        print(\"Error: No video frames provided.\")\n",
    "        return None\n",
    "\n",
    "    # Define the prompt for single concise action phrase\n",
    "    prompt = \"\"\"\n",
    "# --- Task Definition ---\n",
    "You are an expert AI analyzing video frames from a first-person camera worn by a checkout cashier who remains primarily stationary.\n",
    "Your goal is to identify the **single most prominent or representative action** performed by the cashier within the provided video frame sequence.\n",
    "\n",
    "# --- Current Task ---\n",
    "\n",
    "Analyze the following sequence of video frames provided. Identify the single most prominent action and describe it chronologically in a single sentence:\n",
    "\n",
    "*Output Phrase:*\n",
    "\"\"\"\n",
    "\n",
    "    print(f\"Processing {len(video_frames)} frames for Ollama.\")\n",
    "    print(f\"Using model: {model_name}\")\n",
    "\n",
    "    try:\n",
    "        # Convert each PIL Image frame to base64\n",
    "        images_base64 = []\n",
    "        # ... (rest of the function remains the same) ...\n",
    "        for frame in video_frames:\n",
    "            buffer = io.BytesIO()\n",
    "            # Ensure frame is RGB before saving\n",
    "            if frame.mode != 'RGB':\n",
    "                frame = frame.convert('RGB')\n",
    "            frame.save(buffer, format=\"JPEG\") # Or PNG\n",
    "            image_bytes = buffer.getvalue()\n",
    "            images_base64.append(base64.b64encode(image_bytes).decode('utf-8'))\n",
    "\n",
    "        print(f\"Sending request with {len(images_base64)} base64 frames to Ollama...\")\n",
    "\n",
    "        # --- Ollama Interaction with Streaming ---\n",
    "        full_response = \"\"\n",
    "        # Make sure Ollama server is running\n",
    "        for chunk in ollama.chat(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': prompt,\n",
    "                    'images': images_base64 # Pass the LIST of base64 encoded frames\n",
    "                }\n",
    "            ],\n",
    "            stream=True\n",
    "        ):\n",
    "            if 'message' in chunk and 'content' in chunk['message']:\n",
    "                content = chunk['message']['content']\n",
    "                print(content, end='', flush=True)\n",
    "                full_response += content\n",
    "\n",
    "        print(\"\\nCompleted streaming response from Ollama.\")\n",
    "        return full_response\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"Error: The 'ollama' or 'Pillow' library is not installed.\")\n",
    "        print(\"Please install them using: pip install ollama Pillow\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during Ollama interaction: {e}\")\n",
    "        print(\"Please ensure the Ollama server is running and the model is available.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    video_path = \"videos/mono videos/cashier_short (1).mp4\" # Path to your video file\n",
    "    # Set desired frame size and max frames\n",
    "    target_frame_size = (448, 448)\n",
    "    max_frames_to_send = 15 # Limit frames sent to Ollama (e.g., for a 3x3 grid conceptually)\n",
    "\n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"Error: Video file not found at {video_path}\")\n",
    "    else:\n",
    "        # Encode video to get list of PIL frames, resizing them\n",
    "        frames = encode_video(video_path,\n",
    "                              target_size=target_frame_size,\n",
    "                              max_num_frames=max_frames_to_send)\n",
    "\n",
    "        if frames:\n",
    "            # Get description using Ollama\n",
    "            # Use the fp16 model as requested previously\n",
    "            description = get_video_description_ollama(frames, model_name=\"minicpm-v:8b-2.6-fp16\")\n",
    "\n",
    "            if description:\n",
    "                 print(\"\\n--- Final Ollama Description ---\")\n",
    "                 # The description variable already contains the streamed output\n",
    "            else:\n",
    "                 print(\"\\nFailed to get description from Ollama.\")\n",
    "        else:\n",
    "            print(\"Could not encode video frames.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cut videos into n second segments and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving clips to: videos/1.5s clips/cashier\n",
      "Video properties: 1376x1824, FPS: 30.00, Total Frames: 4910, Duration: 163.68s\n",
      "Splitting into 110 clips of approx 1.5s (45 frames) each...\n",
      "Processing clip 1/110: Frames 0 - 44 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_000.mp4 (45 frames)\n",
      "Processing clip 2/110: Frames 45 - 89 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_001.mp4 (45 frames)\n",
      "Processing clip 3/110: Frames 90 - 134 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_002.mp4 (45 frames)\n",
      "Processing clip 4/110: Frames 135 - 179 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_003.mp4 (45 frames)\n",
      "Processing clip 5/110: Frames 180 - 224 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_004.mp4 (45 frames)\n",
      "Processing clip 6/110: Frames 225 - 269 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_005.mp4 (45 frames)\n",
      "Processing clip 7/110: Frames 270 - 314 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_006.mp4 (45 frames)\n",
      "Processing clip 8/110: Frames 315 - 359 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_007.mp4 (45 frames)\n",
      "Processing clip 9/110: Frames 360 - 404 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_008.mp4 (45 frames)\n",
      "Processing clip 10/110: Frames 405 - 449 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_009.mp4 (45 frames)\n",
      "Processing clip 11/110: Frames 450 - 494 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_010.mp4 (45 frames)\n",
      "Processing clip 12/110: Frames 495 - 539 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_011.mp4 (45 frames)\n",
      "Processing clip 13/110: Frames 540 - 584 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_012.mp4 (45 frames)\n",
      "Processing clip 14/110: Frames 585 - 629 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_013.mp4 (45 frames)\n",
      "Processing clip 15/110: Frames 630 - 674 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_014.mp4 (45 frames)\n",
      "Processing clip 16/110: Frames 675 - 719 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_015.mp4 (45 frames)\n",
      "Processing clip 17/110: Frames 720 - 764 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_016.mp4 (45 frames)\n",
      "Processing clip 18/110: Frames 765 - 809 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_017.mp4 (45 frames)\n",
      "Processing clip 19/110: Frames 810 - 854 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_018.mp4 (45 frames)\n",
      "Processing clip 20/110: Frames 855 - 899 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_019.mp4 (45 frames)\n",
      "Processing clip 21/110: Frames 900 - 944 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_020.mp4 (45 frames)\n",
      "Processing clip 22/110: Frames 945 - 989 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_021.mp4 (45 frames)\n",
      "Processing clip 23/110: Frames 990 - 1034 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_022.mp4 (45 frames)\n",
      "Processing clip 24/110: Frames 1035 - 1079 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_023.mp4 (45 frames)\n",
      "Processing clip 25/110: Frames 1080 - 1124 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_024.mp4 (45 frames)\n",
      "Processing clip 26/110: Frames 1125 - 1169 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_025.mp4 (45 frames)\n",
      "Processing clip 27/110: Frames 1170 - 1214 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_026.mp4 (45 frames)\n",
      "Processing clip 28/110: Frames 1215 - 1259 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_027.mp4 (45 frames)\n",
      "Processing clip 29/110: Frames 1260 - 1304 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_028.mp4 (45 frames)\n",
      "Processing clip 30/110: Frames 1305 - 1349 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_029.mp4 (45 frames)\n",
      "Processing clip 31/110: Frames 1350 - 1394 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_030.mp4 (45 frames)\n",
      "Processing clip 32/110: Frames 1395 - 1439 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_031.mp4 (45 frames)\n",
      "Processing clip 33/110: Frames 1440 - 1484 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_032.mp4 (45 frames)\n",
      "Processing clip 34/110: Frames 1485 - 1529 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_033.mp4 (45 frames)\n",
      "Processing clip 35/110: Frames 1530 - 1574 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_034.mp4 (45 frames)\n",
      "Processing clip 36/110: Frames 1575 - 1619 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_035.mp4 (45 frames)\n",
      "Processing clip 37/110: Frames 1620 - 1664 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_036.mp4 (45 frames)\n",
      "Processing clip 38/110: Frames 1665 - 1709 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_037.mp4 (45 frames)\n",
      "Processing clip 39/110: Frames 1710 - 1754 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_038.mp4 (45 frames)\n",
      "Processing clip 40/110: Frames 1755 - 1799 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_039.mp4 (45 frames)\n",
      "Processing clip 41/110: Frames 1800 - 1844 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_040.mp4 (45 frames)\n",
      "Processing clip 42/110: Frames 1845 - 1889 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_041.mp4 (45 frames)\n",
      "Processing clip 43/110: Frames 1890 - 1934 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_042.mp4 (45 frames)\n",
      "Processing clip 44/110: Frames 1935 - 1979 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_043.mp4 (45 frames)\n",
      "Processing clip 45/110: Frames 1980 - 2024 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_044.mp4 (45 frames)\n",
      "Processing clip 46/110: Frames 2025 - 2069 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_045.mp4 (45 frames)\n",
      "Processing clip 47/110: Frames 2070 - 2114 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_046.mp4 (45 frames)\n",
      "Processing clip 48/110: Frames 2115 - 2159 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_047.mp4 (45 frames)\n",
      "Processing clip 49/110: Frames 2160 - 2204 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_048.mp4 (45 frames)\n",
      "Processing clip 50/110: Frames 2205 - 2249 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_049.mp4 (45 frames)\n",
      "Processing clip 51/110: Frames 2250 - 2294 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_050.mp4 (45 frames)\n",
      "Processing clip 52/110: Frames 2295 - 2339 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_051.mp4 (45 frames)\n",
      "Processing clip 53/110: Frames 2340 - 2384 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_052.mp4 (45 frames)\n",
      "Processing clip 54/110: Frames 2385 - 2429 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_053.mp4 (45 frames)\n",
      "Processing clip 55/110: Frames 2430 - 2474 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_054.mp4 (45 frames)\n",
      "Processing clip 56/110: Frames 2475 - 2519 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_055.mp4 (45 frames)\n",
      "Processing clip 57/110: Frames 2520 - 2564 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_056.mp4 (45 frames)\n",
      "Processing clip 58/110: Frames 2565 - 2609 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_057.mp4 (45 frames)\n",
      "Processing clip 59/110: Frames 2610 - 2654 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_058.mp4 (45 frames)\n",
      "Processing clip 60/110: Frames 2655 - 2699 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_059.mp4 (45 frames)\n",
      "Processing clip 61/110: Frames 2700 - 2744 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_060.mp4 (45 frames)\n",
      "Processing clip 62/110: Frames 2745 - 2789 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_061.mp4 (45 frames)\n",
      "Processing clip 63/110: Frames 2790 - 2834 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_062.mp4 (45 frames)\n",
      "Processing clip 64/110: Frames 2835 - 2879 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_063.mp4 (45 frames)\n",
      "Processing clip 65/110: Frames 2880 - 2924 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_064.mp4 (45 frames)\n",
      "Processing clip 66/110: Frames 2925 - 2969 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_065.mp4 (45 frames)\n",
      "Processing clip 67/110: Frames 2970 - 3014 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_066.mp4 (45 frames)\n",
      "Processing clip 68/110: Frames 3015 - 3059 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_067.mp4 (45 frames)\n",
      "Processing clip 69/110: Frames 3060 - 3104 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_068.mp4 (45 frames)\n",
      "Processing clip 70/110: Frames 3105 - 3149 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_069.mp4 (45 frames)\n",
      "Processing clip 71/110: Frames 3150 - 3194 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_070.mp4 (45 frames)\n",
      "Processing clip 72/110: Frames 3195 - 3239 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_071.mp4 (45 frames)\n",
      "Processing clip 73/110: Frames 3240 - 3284 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_072.mp4 (45 frames)\n",
      "Processing clip 74/110: Frames 3285 - 3329 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_073.mp4 (45 frames)\n",
      "Processing clip 75/110: Frames 3330 - 3374 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_074.mp4 (45 frames)\n",
      "Processing clip 76/110: Frames 3375 - 3419 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_075.mp4 (45 frames)\n",
      "Processing clip 77/110: Frames 3420 - 3464 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_076.mp4 (45 frames)\n",
      "Processing clip 78/110: Frames 3465 - 3509 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_077.mp4 (45 frames)\n",
      "Processing clip 79/110: Frames 3510 - 3554 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_078.mp4 (45 frames)\n",
      "Processing clip 80/110: Frames 3555 - 3599 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_079.mp4 (45 frames)\n",
      "Processing clip 81/110: Frames 3600 - 3644 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_080.mp4 (45 frames)\n",
      "Processing clip 82/110: Frames 3645 - 3689 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_081.mp4 (45 frames)\n",
      "Processing clip 83/110: Frames 3690 - 3734 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_082.mp4 (45 frames)\n",
      "Processing clip 84/110: Frames 3735 - 3779 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_083.mp4 (45 frames)\n",
      "Processing clip 85/110: Frames 3780 - 3824 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_084.mp4 (45 frames)\n",
      "Processing clip 86/110: Frames 3825 - 3869 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_085.mp4 (45 frames)\n",
      "Processing clip 87/110: Frames 3870 - 3914 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_086.mp4 (45 frames)\n",
      "Processing clip 88/110: Frames 3915 - 3959 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_087.mp4 (45 frames)\n",
      "Processing clip 89/110: Frames 3960 - 4004 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_088.mp4 (45 frames)\n",
      "Processing clip 90/110: Frames 4005 - 4049 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_089.mp4 (45 frames)\n",
      "Processing clip 91/110: Frames 4050 - 4094 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_090.mp4 (45 frames)\n",
      "Processing clip 92/110: Frames 4095 - 4139 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_091.mp4 (45 frames)\n",
      "Processing clip 93/110: Frames 4140 - 4184 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_092.mp4 (45 frames)\n",
      "Processing clip 94/110: Frames 4185 - 4229 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_093.mp4 (45 frames)\n",
      "Processing clip 95/110: Frames 4230 - 4274 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_094.mp4 (45 frames)\n",
      "Processing clip 96/110: Frames 4275 - 4319 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_095.mp4 (45 frames)\n",
      "Processing clip 97/110: Frames 4320 - 4364 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_096.mp4 (45 frames)\n",
      "Processing clip 98/110: Frames 4365 - 4409 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_097.mp4 (45 frames)\n",
      "Processing clip 99/110: Frames 4410 - 4454 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_098.mp4 (45 frames)\n",
      "Processing clip 100/110: Frames 4455 - 4499 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_099.mp4 (45 frames)\n",
      "Processing clip 101/110: Frames 4500 - 4544 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_100.mp4 (45 frames)\n",
      "Processing clip 102/110: Frames 4545 - 4589 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_101.mp4 (45 frames)\n",
      "Processing clip 103/110: Frames 4590 - 4634 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_102.mp4 (45 frames)\n",
      "Processing clip 104/110: Frames 4635 - 4679 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_103.mp4 (45 frames)\n",
      "Processing clip 105/110: Frames 4680 - 4724 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_104.mp4 (45 frames)\n",
      "Processing clip 106/110: Frames 4725 - 4769 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_105.mp4 (45 frames)\n",
      "Processing clip 107/110: Frames 4770 - 4814 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_106.mp4 (45 frames)\n",
      "Processing clip 108/110: Frames 4815 - 4859 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_107.mp4 (45 frames)\n",
      "Processing clip 109/110: Frames 4860 - 4904 (45 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_108.mp4 (45 frames)\n",
      "Processing clip 110/110: Frames 4905 - 4909 (5 frames)\n",
      " > Saved: videos/1.5s clips/cashier/cashier_clip_109.mp4 (5 frames)\n",
      "Video splitting complete.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import math\n",
    "\n",
    "# --- Configuration ---\n",
    "input_video_path = \"videos/mono videos/cashier.mov\" # Path to your source video\n",
    "base_output_dir = \"videos/1.5s clips\"          # Base directory to store clip folders\n",
    "clip_length_seconds = 1.5                     # Duration of each clip\n",
    "\n",
    "# --- Helper Function using OpenCV ---\n",
    "def split_video_opencv(video_path, output_dir_base, clip_duration):\n",
    "    \"\"\"\n",
    "    Splits a video into clips of a specified duration using OpenCV.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): Path to the input video file.\n",
    "        output_dir_base (str): The base directory where the clip folder will be created.\n",
    "        clip_duration (float): The duration of each clip in seconds.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"Error: Input video not found at {video_path}\")\n",
    "        return\n",
    "\n",
    "    # Extract video base name for the subdirectory\n",
    "    video_filename = os.path.basename(video_path)\n",
    "    video_base_name, _ = os.path.splitext(video_filename)\n",
    "    specific_output_dir = os.path.join(output_dir_base, video_base_name)\n",
    "\n",
    "    # Create the specific output directory if it doesn't exist\n",
    "    os.makedirs(specific_output_dir, exist_ok=True)\n",
    "    print(f\"Saving clips to: {specific_output_dir}\")\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file {video_path} with OpenCV.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Get video properties\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        duration = total_frames / fps if fps > 0 else 0\n",
    "\n",
    "        if fps <= 0 or total_frames <= 0:\n",
    "            print(f\"Error: Invalid video properties (FPS: {fps}, Frames: {total_frames}). Cannot split.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Video properties: {frame_width}x{frame_height}, FPS: {fps:.2f}, Total Frames: {total_frames}, Duration: {duration:.2f}s\")\n",
    "\n",
    "        # Calculate frames per clip\n",
    "        frames_per_clip = int(math.ceil(fps * clip_duration))\n",
    "        num_clips = math.ceil(total_frames / frames_per_clip)\n",
    "        print(f\"Splitting into {num_clips} clips of approx {clip_duration}s ({frames_per_clip} frames) each...\")\n",
    "\n",
    "        # Define the codec (FourCC) - 'mp4v' is common for .mp4 files\n",
    "        # Other options: 'XVID', 'MJPG' (for .avi), etc.\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "        # Process each clip\n",
    "        current_frame = 0\n",
    "        for i in range(num_clips):\n",
    "            start_frame = i * frames_per_clip\n",
    "            # Ensure end_frame doesn't exceed total_frames\n",
    "            end_frame = min((i + 1) * frames_per_clip, total_frames)\n",
    "\n",
    "            # Check if the clip has any frames\n",
    "            if start_frame >= total_frames:\n",
    "                print(f\"Skipping clip {i+1} as start frame {start_frame} is beyond total frames {total_frames}\")\n",
    "                continue\n",
    "\n",
    "            clip_frame_count = end_frame - start_frame\n",
    "            if clip_frame_count <= 0:\n",
    "                 print(f\"Skipping clip {i+1} as it has no frames.\")\n",
    "                 continue\n",
    "\n",
    "            print(f\"Processing clip {i+1}/{num_clips}: Frames {start_frame} - {end_frame-1} ({clip_frame_count} frames)\")\n",
    "\n",
    "            # Define output filename\n",
    "            output_filename = f\"{video_base_name}_clip_{i:03d}.mp4\"\n",
    "            output_filepath = os.path.join(specific_output_dir, output_filename)\n",
    "\n",
    "            # Create VideoWriter for the current clip\n",
    "            writer = cv2.VideoWriter(output_filepath, fourcc, fps, (frame_width, frame_height))\n",
    "            if not writer.isOpened():\n",
    "                print(f\"   ! Error: Could not open VideoWriter for {output_filepath}\")\n",
    "                continue # Skip to the next clip\n",
    "\n",
    "            frames_written = 0\n",
    "            try:\n",
    "                # Seek to the start frame of the clip\n",
    "                # Note: Precise seeking can still be an issue, but we read sequentially within the clip\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "                for frame_num in range(start_frame, end_frame):\n",
    "                     # Check current position just in case seeking wasn't perfect\n",
    "                    current_pos = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "                    # If we are too far ahead or behind where we should be, break (might indicate read error)\n",
    "                    if abs(current_pos - frame_num) > fps: # Allow some tolerance\n",
    "                         print(f\"   ! Warning: Frame position mismatch (Expected: {frame_num}, Got: {current_pos}). Stopping write for this clip.\")\n",
    "                         break\n",
    "\n",
    "                    ret, frame = cap.read()\n",
    "                    if not ret:\n",
    "                        print(f\"   ! Warning: Could not read frame {frame_num}. Stopping write for this clip.\")\n",
    "                        break # Stop writing if a frame can't be read\n",
    "\n",
    "                    writer.write(frame)\n",
    "                    frames_written += 1\n",
    "\n",
    "                print(f\" > Saved: {output_filepath} ({frames_written} frames)\")\n",
    "\n",
    "            except Exception as write_error:\n",
    "                 print(f\"   ! Error writing clip {i+1}: {write_error}\")\n",
    "            finally:\n",
    "                if writer.isOpened():\n",
    "                    writer.release() # Release the writer for this clip\n",
    "\n",
    "        print(\"Video splitting complete.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during OpenCV video processing: {e}\")\n",
    "    finally:\n",
    "        if cap.isOpened():\n",
    "            cap.release() # Ensure the main video capture is released\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    split_video_opencv(input_video_path, base_output_dir, clip_length_seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annotate video using k_shot video-text pairs with Minicpm-v:8b-2.6-fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frames: 45. Attempting to extract up to 1 frames from indices: [np.int64(0)]\n",
      "Frames processed for model (resized to (448, 448)): 1\n",
      "Processing 3 k-shot examples...\n",
      "  Processing example: cashier/cashier_clip_006.mp4\n",
      "Total frames: 45. Attempting to extract up to 1 frames from indices: [np.int64(0)]\n",
      "Frames processed for model (resized to (448, 448)): 1\n",
      "    > Example cashier/cashier_clip_006.mp4 added successfully (1 frames).\n",
      "  Processing example: cashier/cashier_clip_066.mp4\n",
      "Total frames: 45. Attempting to extract up to 1 frames from indices: [np.int64(0)]\n",
      "Frames processed for model (resized to (448, 448)): 1\n",
      "    > Example cashier/cashier_clip_066.mp4 added successfully (1 frames).\n",
      "  Processing example: cashier/cashier_clip_102.mp4\n",
      "Total frames: 45. Attempting to extract up to 1 frames from indices: [np.int64(0)]\n",
      "Frames processed for model (resized to (448, 448)): 1\n",
      "    > Example cashier/cashier_clip_102.mp4 added successfully (1 frames).\n",
      "Finished processing k-shot examples. 3 examples successfully prepared.\n",
      "Processing 1 frames for Ollama.\n",
      "Using model: gemma3:27b\n",
      "Sending request with 1 base64 frames to Ollama...\n",
      " gegen\"\n",
      "\n",
      "Completed streaming response from Ollama.\n",
      "\n",
      "--- Final Ollama Description ---\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "import cv2          # Import OpenCV\n",
    "import numpy as np  # Import numpy\n",
    "import json\n",
    "# --- Video Encoding Function (Using OpenCV) ---\n",
    "\n",
    "def encode_video(video_path, target_size=(448, 448), num_frames_to_extract=6):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file {video_path} with OpenCV.\")\n",
    "        return None\n",
    "\n",
    "    frames_pil = []\n",
    "    try:\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        # Basic check for valid video length\n",
    "        if total_frames <= 0 or num_frames_to_extract <= 0:\n",
    "            print(f\"Warning: Invalid frame count ({total_frames}) or requested frames ({num_frames_to_extract}).\")\n",
    "            cap.release()\n",
    "            return [] # Return empty list\n",
    "\n",
    "        # Calculate indices for uniform sampling\n",
    "        # Use np.linspace to get evenly spaced indices, rounding to nearest int\n",
    "        indices_float = np.linspace(0, total_frames - 1, num_frames_to_extract)\n",
    "        frame_indices_to_sample = np.round(indices_float).astype(int)\n",
    "        # Clip indices to be within valid range [0, total_frames - 1]\n",
    "        frame_indices_to_sample = np.clip(frame_indices_to_sample, 0, total_frames - 1)\n",
    "        # Remove potential duplicates caused by rounding, although keep order\n",
    "        frame_indices_to_sample = sorted(list(set(frame_indices_to_sample)))\n",
    "\n",
    "\n",
    "        print(f\"Total frames: {total_frames}. Attempting to extract up to {num_frames_to_extract} frames from indices: {frame_indices_to_sample}\")\n",
    "\n",
    "        # Read, resize, and convert selected frames\n",
    "        for frame_index in frame_indices_to_sample:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                resized_frame = cv2.resize(frame, target_size, interpolation=cv2.INTER_AREA)\n",
    "                frame_rgb = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB)\n",
    "                frames_pil.append(Image.fromarray(frame_rgb))\n",
    "            else:\n",
    "                # Just warn if a specific frame fails, continue trying others\n",
    "                print(f\"Warning: Could not read frame at index {frame_index}\")\n",
    "\n",
    "        print(f'Frames processed for model (resized to {target_size}): {len(frames_pil)}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during OpenCV video processing: {e}\")\n",
    "        # frames_pil might contain partial results, or be empty\n",
    "    finally:\n",
    "        if cap.isOpened():\n",
    "            cap.release()\n",
    "\n",
    "    return frames_pil\n",
    "# --- Function to get video description using Ollama ---\n",
    "# ... (get_video_description_ollama function remains the same) ...\n",
    "def create_k_shot_prompt(path_to_k_shots, num_frames):\n",
    "    # Load k-shot examples from JSON\n",
    "    try:\n",
    "        with open(path_to_k_shots, 'r') as f:\n",
    "            k_shots = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: K-shot JSON file not found at {path_to_k_shots}\")\n",
    "        return [] # Return empty list if JSON is missing\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Could not decode JSON from {path_to_k_shots}\")\n",
    "        return []\n",
    "\n",
    "    msgs = []\n",
    "    n = 0\n",
    "    print(f\"Processing {len(k_shots)} k-shot examples...\")\n",
    "    # Process each video and create k-shot examples\n",
    "    for video_file, description in k_shots.items():\n",
    "        video_path = os.path.join(os.path.dirname(path_to_k_shots), video_file)\n",
    "        print(f\"  Processing example: {video_file}\")\n",
    "\n",
    "        # Get frames from the example video\n",
    "        frames = encode_video(video_path, target_size=(448, 448), num_frames_to_extract=num_frames) # Use a low frame count for examples\n",
    "\n",
    "        # --- Check if frames were successfully encoded ---\n",
    "        if frames:\n",
    "            # --- Correctly format for Ollama API ---\n",
    "            try:\n",
    "                images_base64 = []\n",
    "                for frame in frames:\n",
    "                    buffer = io.BytesIO()\n",
    "                    if frame.mode != 'RGB':\n",
    "                        frame = frame.convert('RGB')\n",
    "                    frame.save(buffer, format=\"JPEG\")\n",
    "                    image_bytes = buffer.getvalue()\n",
    "                    images_base64.append(base64.b64encode(image_bytes).decode('utf-8'))\n",
    "\n",
    "                # Add the correctly formatted user/assistant pair\n",
    "                msgs.append({'role': 'user',\n",
    "                             'content': f'Example {n}', # Text content\n",
    "                            #  'content': 'Provide a description of the single main action performed by the cashier wearing the first person view camera. Use the examples and the follwing rules \\n1. Output **only one single phrase**.\\n2. The phrase must start with \\\"the person\\\".\\n3. Describe the single chosen action concisely (e.g., reaching, grabbing, placing, scanning, tapping, looking, waiting).\\n4. Mention the hand used (e.g., \\\"with their right hand\\\", \\\"with their left hand\\\", \\\"with both hands\\\") if the action involves a specific hand.',\n",
    "                             'images': images_base64})               # List of base64 images\n",
    "                msgs.append({'role': 'assistant',\n",
    "                             'content': description})                # Assistant response is just the text\n",
    "                print(f\"    > Example {video_file} added successfully ({len(images_base64)} frames).\")\n",
    "                n+=1\n",
    "            except Exception as e:\n",
    "                 print(f\"    ! Error encoding frames for {video_file}: {e}\")\n",
    "                 # Optionally skip this example if encoding fails\n",
    "                 continue\n",
    "        else:\n",
    "            # Skip this example if encode_video failed (e.g., file not found/corrupt)\n",
    "            print(f\"    ! Skipping example {video_file} as frames could not be encoded.\")\n",
    "            continue # Move to the next example\n",
    "\n",
    "    print(f\"Finished processing k-shot examples. {len(msgs)//2} examples successfully prepared.\")\n",
    "    return msgs\n",
    "\n",
    "# gemma3:27b\n",
    "# minicpm-v:8b-2.6-fp16\n",
    "\n",
    "\n",
    "def get_video_description_ollama(video_frames, k_shot_prompt, model_name=\"gemma3:27b\"):\n",
    "\n",
    "    if not video_frames:\n",
    "        print(\"Error: No video frames provided.\")\n",
    "        return None\n",
    "\n",
    "    # ... (prompt definition remains the same) ...\n",
    "    # No longer needed here, handled in message construction below\n",
    "    # prompt = \"\"\"...\"\"\"\n",
    "\n",
    "    print(f\"Processing {len(video_frames)} frames for Ollama.\")\n",
    "    print(f\"Using model: {model_name}\")\n",
    "\n",
    "    try:\n",
    "        # Convert each PIL Image frame to base64\n",
    "        images_base64 = []\n",
    "        for frame in video_frames:\n",
    "            # ... (frame conversion to base64 remains the same) ...\n",
    "            buffer = io.BytesIO()\n",
    "            if frame.mode != 'RGB':\n",
    "                frame = frame.convert('RGB')\n",
    "            frame.save(buffer, format=\"JPEG\") # Or PNG\n",
    "            image_bytes = buffer.getvalue()\n",
    "            images_base64.append(base64.b64encode(image_bytes).decode('utf-8'))\n",
    "\n",
    "        print(f\"Sending request with {len(images_base64)} base64 frames to Ollama...\")\n",
    "\n",
    "        # Define the system instruction\n",
    "        instruction = \"\"\"Below are 3 examples of image sequences along with their first person view description\"\"\"\n",
    "\n",
    "        # Construct the final message list\n",
    "        # System Instruction + K-Shot Examples + Final User Query\n",
    "        final_user_message = {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"Provide a description of the single main action performed by the cashier wearing the first person view camera. Use the examples and the follwing rules \\n1. Output **only one single phrase**.\\n2. The phrase must start with \\\"the person\\\".\\n3. Describe the single chosen action concisely (e.g., reaching, grabbing, placing, scanning, tapping, looking, waiting).\\n4. Mention the hand used (e.g., \\\"with their right hand\\\", \\\"with their left hand\\\", \\\"with both hands\\\") if the action involves a specific hand. Do not describe or care about what the other persons in the scene are doing, you are the cashier only\",\n",
    "            \"images\": images_base64                 # <-- List of base64 strings goes here\n",
    "        }\n",
    "        msg = [{\"role\": \"user\", \"content\": instruction}] + k_shot_prompt + [final_user_message]\n",
    "        # Print full message for debugging\n",
    "        # print(\"\\n=== Full Message ===\")\n",
    "        # print(json.dumps(msg, indent=2))\n",
    "        # print(\"===================\\n\")\n",
    "        \n",
    "        # --- Ollama Interaction with Streaming ---\n",
    "        full_response = \"\"\n",
    "        # Make sure Ollama server is running\n",
    "        for chunk in ollama.chat(\n",
    "            model=model_name,\n",
    "            messages=msg, # Use the correctly constructed message list\n",
    "            stream=True\n",
    "        #     options={\n",
    "        #     \"temperature\": 0.9,  # Lowers randomness, encourages sticking to learned patterns/facts\n",
    "        #     \"num_predict\": 80,   # Limits max output length, suitable for concise phrases\n",
    "        #     # \"top_k\": 20,         # Reduces the pool of possible next words, enhancing focus\n",
    "        #     # \"top_p\": 0.8,      # Alternative/addition to top_k, could try if top_k isn't enough\n",
    "        #     \"stop\": [\"\\n\"]     # Stop generation immediately if it tries to start a new line\n",
    "        # }\n",
    "        ):\n",
    "            if 'message' in chunk and 'content' in chunk['message']:\n",
    "                content = chunk['message']['content']\n",
    "                print(content, end='', flush=True)\n",
    "                full_response += content\n",
    "\n",
    "        print(\"\\nCompleted streaming response from Ollama.\")\n",
    "        return full_response\n",
    "\n",
    "    # ... (exception handling remains the same) ...\n",
    "    except ImportError:\n",
    "        print(\"Error: The 'ollama' or 'Pillow' library is not installed.\")\n",
    "        print(\"Please install them using: pip install ollama Pillow\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during Ollama interaction: {e}\")\n",
    "        print(\"Please ensure the Ollama server is running and the model is available.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_path = \"videos/1.5s clips/cashier/cashier_clip_094.mp4\" # Path to your video file\n",
    "    path_to_k_shots = \"videos/1.5s clips/k_shots.json\"\n",
    "    # Set desired frame size and max frames\n",
    "    target_frame_size = (448, 448)\n",
    "    frames_to_send = 1 # Limit frames sent to Ollama (e.g., for a 3x3 grid conceptually)\n",
    "\n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"Error: Video file not found at {video_path}\")\n",
    "    else:\n",
    "        # Encode video to get list of PIL frames, resizing them\n",
    "        frames = encode_video(video_path,\n",
    "                              target_size=target_frame_size,\n",
    "                              num_frames_to_extract=frames_to_send)\n",
    "\n",
    "        if frames:\n",
    "            # Get description using Ollama\n",
    "            # Use the fp16 model as requested previously\n",
    "            # Assuming create_k_shot_prompt is defined correctly elsewhere or inline\n",
    "            k_shot_prompt = create_k_shot_prompt(path_to_k_shots, num_frames=frames_to_send) # Make sure this function is defined and correct\n",
    "            if k_shot_prompt is not None: # Check if k-shot prompt was created successfully\n",
    "                description = get_video_description_ollama(frames, k_shot_prompt, model_name=\"gemma3:27b\")\n",
    "\n",
    "                if description:\n",
    "                     print(\"\\n--- Final Ollama Description ---\")\n",
    "                     # The description variable already contains the streamed output\n",
    "                else:\n",
    "                     print(\"\\nFailed to get description from Ollama.\")\n",
    "            else:\n",
    "                print(\"\\nFailed to create k-shot prompt.\")\n",
    "        else:\n",
    "            print(\"Could not encode video frames.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
