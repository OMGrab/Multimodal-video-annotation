{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point pro "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ml-depth-pro'...\n",
      "remote: Enumerating objects: 45, done.\u001b[K\n",
      "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
      "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
      "remote: Total 45 (delta 7), reused 2 (delta 2), pack-reused 21 (from 1)\u001b[K\n",
      "Receiving objects: 100% (45/45), 2.50 MiB | 9.81 MiB/s, done.\n",
      "Resolving deltas: 100% (7/7), done.\n",
      "/Users/hendrik/.Trash/ml-depth-pro/ml-depth-pro/ml-depth-pro\n",
      "Obtaining file:///Users/hendrik/.Trash/ml-depth-pro/ml-depth-pro/ml-depth-pro\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from depth_pro==0.1) (2.6.0)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from depth_pro==0.1) (0.21.0)\n",
      "Requirement already satisfied: timm in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from depth_pro==0.1) (1.0.15)\n",
      "Requirement already satisfied: numpy<2 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from depth_pro==0.1) (1.26.4)\n",
      "Requirement already satisfied: pillow_heif in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from depth_pro==0.1) (0.22.0)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from depth_pro==0.1) (3.10.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from matplotlib->depth_pro==0.1) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from matplotlib->depth_pro==0.1) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from matplotlib->depth_pro==0.1) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from matplotlib->depth_pro==0.1) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from matplotlib->depth_pro==0.1) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from matplotlib->depth_pro==0.1) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from matplotlib->depth_pro==0.1) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from matplotlib->depth_pro==0.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from timm->depth_pro==0.1) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from timm->depth_pro==0.1) (0.29.3)\n",
      "Requirement already satisfied: safetensors in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from timm->depth_pro==0.1) (0.5.3)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from torch->depth_pro==0.1) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from torch->depth_pro==0.1) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from torch->depth_pro==0.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from torch->depth_pro==0.1) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from torch->depth_pro==0.1) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from torch->depth_pro==0.1) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from torch->depth_pro==0.1) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from sympy==1.13.1->torch->depth_pro==0.1) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->depth_pro==0.1) (1.16.0)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from huggingface_hub->timm->depth_pro==0.1) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from huggingface_hub->timm->depth_pro==0.1) (4.67.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from jinja2->torch->depth_pro==0.1) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from requests->huggingface_hub->timm->depth_pro==0.1) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from requests->huggingface_hub->timm->depth_pro==0.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from requests->huggingface_hub->timm->depth_pro==0.1) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from requests->huggingface_hub->timm->depth_pro==0.1) (2024.8.30)\n",
      "Building wheels for collected packages: depth_pro\n",
      "  Building editable for depth_pro (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for depth_pro: filename=depth_pro-0.1-0.editable-py3-none-any.whl size=4825 sha256=68672ec1bd914f568c4b16640d7fa41b3659f3db3460dffa8554c5232ab83ece\n",
      "  Stored in directory: /private/var/folders/1g/rwnkwjjs0w9_xcc0cwsf40dw0000gn/T/pip-ephem-wheel-cache-rgp6uhqm/wheels/2a/14/54/346db838b987cddd8df84bd1491e99517b9a3890cc2bffe77b\n",
      "Successfully built depth_pro\n",
      "Installing collected packages: depth_pro\n",
      "  Attempting uninstall: depth_pro\n",
      "    Found existing installation: depth_pro 0.1\n",
      "    Uninstalling depth_pro-0.1:\n",
      "      Successfully uninstalled depth_pro-0.1\n",
      "Successfully installed depth_pro-0.1\n",
      "--2025-04-13 12:59:34--  https://ml-site.cdn-apple.com/models/depth-pro/depth_pro.pt\n",
      "Resolving ml-site.cdn-apple.com (ml-site.cdn-apple.com)... 2620:149:a0c:f100::4, 2620:149:a0c:f000::1, 17.253.5.202, ...\n",
      "Connecting to ml-site.cdn-apple.com (ml-site.cdn-apple.com)|2620:149:a0c:f100::4|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1904446787 (1.8G) [binary/octet-stream]\n",
      "Saving to: ‘checkpoints/depth_pro.pt’\n",
      "\n",
      "depth_pro.pt        100%[===================>]   1.77G  55.1MB/s    in 31s     \n",
      "\n",
      "2025-04-13 13:00:05 (58.7 MB/s) - ‘checkpoints/depth_pro.pt’ saved [1904446787/1904446787]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/apple/ml-depth-pro.git\n",
    "%cd ml-depth-pro\n",
    "\n",
    "# 2. Install the package\n",
    "!pip install -e .\n",
    "\n",
    "# 3. Download the pretrained models\n",
    "!bash get_pretrained_models.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single image to point cloud with estimated Focal length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU (MPS) is available and will be used\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ml-depth-pro/checkpoints/depth_pro.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 180\u001b[0m\n\u001b[1;32m    176\u001b[0m device \u001b[38;5;241m=\u001b[39m setup_device()\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# --- Assumptions: These functions exist in your depth_pro module ---\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# Load the depth estimation model\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m model, transform \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model_and_transforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mDEFAULT_MONODEPTH_CONFIG_DICT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    183\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/OMGrab/ml-depth-pro/src/depth_pro/depth_pro.py:135\u001b[0m, in \u001b[0;36mcreate_model_and_transforms\u001b[0;34m(config, device, precision)\u001b[0m\n\u001b[1;32m    125\u001b[0m transform \u001b[38;5;241m=\u001b[39m Compose(\n\u001b[1;32m    126\u001b[0m     [\n\u001b[1;32m    127\u001b[0m         ToTensor(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    131\u001b[0m     ]\n\u001b[1;32m    132\u001b[0m )\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mcheckpoint_uri \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     missing_keys, unexpected_keys \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mload_state_dict(\n\u001b[1;32m    137\u001b[0m         state_dict\u001b[38;5;241m=\u001b[39mstate_dict, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     )\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unexpected_keys) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.12/site-packages/torch/serialization.py:1425\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1423\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1425\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1427\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1428\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1429\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1430\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.12/site-packages/torch/serialization.py:751\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    750\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 751\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.12/site-packages/torch/serialization.py:732\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 732\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ml-depth-pro/checkpoints/depth_pro.pt'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./ml-depth-pro')\n",
    "from src.depth_pro.depth_pro import *\n",
    "from src.depth_pro.utils import *\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import open3d as o3d\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "def setup_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"GPU (CUDA) is available and will be used\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"GPU (MPS) is available and will be used\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"GPU is not available, using CPU\")\n",
    "    return device\n",
    "\n",
    "def get_image_paths_from_directory(directory, extensions=(\".jpg\", \".jpeg\", \".png\")):\n",
    "    \"\"\"Fetch all image file paths from the given directory.\"\"\"\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory) if f.lower().endswith(extensions)]\n",
    "\n",
    "def load_and_process_images_from_directory(directory, transform, device):\n",
    "    \"\"\"Load and preprocess all images from a directory.\"\"\"\n",
    "    image_paths = get_image_paths_from_directory(directory)\n",
    "    \n",
    "    images_tensors = []\n",
    "    focal_lengths = []\n",
    "    \n",
    "    for image_path in image_paths:\n",
    "        image, _, f_px = depth_pro.load_rgb(image_path)\n",
    "        images_tensors.append(transform(image).to(device))\n",
    "        focal_lengths.append(f_px)\n",
    "    \n",
    "    return image_paths, images_tensors, focal_lengths\n",
    "\n",
    "def run_depth_inference(model, image_tensors, focal_lengths):\n",
    "    \"\"\"Run depth inference on multiple images.\"\"\"\n",
    "    depth_maps = []\n",
    "    focal_px_values = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for image_tensor, f_px in zip(image_tensors, focal_lengths):\n",
    "            prediction = model.infer(image_tensor, f_px=f_px)\n",
    "            depth_maps.append(prediction[\"depth\"].cpu().numpy())\n",
    "            focal_px_values.append(prediction[\"focallength_px\"].item())\n",
    "    \n",
    "    return depth_maps, focal_px_values\n",
    "\n",
    "def create_point_clouds(depth_maps, images, focal_px_values, scale_ratio=1.0):\n",
    "    \"\"\"Create multiple point clouds from depth maps and images.\"\"\"\n",
    "    point_clouds = []\n",
    "    \n",
    "    for depth_map, image, focal_px in zip(depth_maps, images, focal_px_values):\n",
    "        H, W = depth_map.shape\n",
    "        image = ensure_image_format(image)\n",
    "        \n",
    "        x_grid, y_grid = np.meshgrid(np.arange(W), np.arange(H))\n",
    "        cx, cy = W / 2, H / 2\n",
    "\n",
    "        Z = depth_map * scale_ratio\n",
    "        X = (x_grid - cx) * Z / focal_px\n",
    "        Y = (y_grid - cy) * Z / focal_px\n",
    "\n",
    "        point_map = np.stack((X, Y, Z), axis=-1)\n",
    "\n",
    "        point_cloud = o3d.geometry.PointCloud()\n",
    "        point_cloud.points = o3d.utility.Vector3dVector(point_map.reshape(-1, 3))\n",
    "        point_cloud.colors = o3d.utility.Vector3dVector(image.reshape(-1, 3))\n",
    "\n",
    "        valid_points = Z.flatten() > 0\n",
    "        point_cloud.points = o3d.utility.Vector3dVector(np.asarray(point_cloud.points)[valid_points])\n",
    "        point_cloud.colors = o3d.utility.Vector3dVector(np.asarray(point_cloud.colors)[valid_points])\n",
    "\n",
    "        point_clouds.append(point_cloud)\n",
    "    \n",
    "    return point_clouds\n",
    "\n",
    "def ensure_image_format(image):\n",
    "    \"\"\"Ensure image is in the correct format (H, W, 3) and properly scaled.\"\"\"\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        image = image.cpu().detach().numpy()\n",
    "    \n",
    "    if image.shape[0] == 3:  # If in (3, H, W) format\n",
    "        image = np.transpose(image, (1, 2, 0))\n",
    "    \n",
    "    if image.min() < 0 or image.max() > 1:\n",
    "        image = (image + 1) / 2  # Normalize to [0,1]\n",
    "    \n",
    "    return np.clip(image, 0, 1)\n",
    "\n",
    "def visualize_results(images, depth_maps):\n",
    "    \"\"\"Visualize multiple images and depth maps.\"\"\"\n",
    "    num_images = len(images)\n",
    "    fig, axes = plt.subplots(num_images, 2, figsize=(10, 5 * num_images))\n",
    "    \n",
    "    if num_images == 1:\n",
    "        axes = [axes]  # Make it iterable for a single image\n",
    "    \n",
    "    for i, (image, depth_map) in enumerate(zip(images, depth_maps)):\n",
    "        axes[i][0].imshow(image)\n",
    "        axes[i][0].set_title(f\"Original Image {i+1}\")\n",
    "        axes[i][0].axis('off')\n",
    "\n",
    "        depth_plot = axes[i][1].imshow(depth_map, cmap='viridis')\n",
    "        axes[i][1].set_title(f\"Depth Map {i+1}\")\n",
    "        fig.colorbar(depth_plot, ax=axes[i][1], label='Depth (m)')\n",
    "        axes[i][1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def save_point_clouds(point_clouds, image_paths):\n",
    "    \"\"\"Save multiple point clouds to 'generated_pointclouds/'.\"\"\"\n",
    "    output_dir = \"generated_pointclouds\"\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "    for point_cloud, image_path in zip(point_clouds, image_paths):\n",
    "        filename = os.path.basename(image_path).split('.')[0] + \".ply\"\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        o3d.io.write_point_cloud(output_path, point_cloud)\n",
    "        print(f\"Saved: {output_path}\")\n",
    "\n",
    "def calculate_distances(depth_maps, points_of_interest=None):\n",
    "    \"\"\"\n",
    "    Calculate absolute metric distances from the camera to objects in multiple depth maps.\n",
    "    \n",
    "    Parameters:\n",
    "    depth_maps (list of np.array): List of depth maps generated by Depth Pro.\n",
    "    points_of_interest (list of lists): Optional. List of lists, where each sublist contains\n",
    "                                        (y, x) coordinates of specific points to measure.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing distance statistics or point-specific distances for each depth map.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for i, depth_map in enumerate(depth_maps):\n",
    "        if points_of_interest is None:\n",
    "            valid_depths = depth_map[depth_map > 0]  # Ignore zero or negative depths\n",
    "            results[f'image_{i+1}'] = {\n",
    "                'min_distance': np.min(valid_depths),\n",
    "                'max_distance': np.max(valid_depths),\n",
    "                'mean_distance': np.mean(valid_depths)\n",
    "            }\n",
    "        else:\n",
    "            distances = {}\n",
    "            for j, (y, x) in enumerate(points_of_interest[i]):\n",
    "                distance = depth_map[y, x]\n",
    "                distances[f'point_{j}'] = distance if distance > 0 else None\n",
    "            results[f'image_{i+1}'] = distances\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    DEFAULT_MONODEPTH_CONFIG_DICT = DepthProConfig(\n",
    "    patch_encoder_preset=\"dinov2l16_384\",\n",
    "    image_encoder_preset=\"dinov2l16_384\",\n",
    "    checkpoint_uri= \"ml-depth-pro/checkpoints/depth_pro.pt\",\n",
    "    decoder_features=256,\n",
    "    use_fov_head=True,\n",
    "    fov_encoder_preset=\"dinov2l16_384\",\n",
    "    )\n",
    "    # 1. Setup Device\n",
    "    device = setup_device()\n",
    "\n",
    "    # --- Assumptions: These functions exist in your depth_pro module ---\n",
    "    # Load the depth estimation model\n",
    "    model, transform = create_model_and_transforms(config = DEFAULT_MONODEPTH_CONFIG_DICT)\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    # Load the necessary image transforms    # --- End Assumptions ---\n",
    "\n",
    "    # 2. Define Image Paths\n",
    "    image_paths = [\n",
    "        # \"/Users/hendrik/OMGrab/output_grid.jpg\",\n",
    "        # Add the right image if you have it, e.g.:\n",
    "        \"/Users/hendrik/OMGrab/Stereo images/right_motorcycle.jpg\"\n",
    "    ]\n",
    "    checkpoint_path = \"./ml-depth-pro/checkpoints/depth_pro.pt\"\n",
    "\n",
    "    if not all(os.path.exists(p) for p in image_paths):\n",
    "        print(\"Error: One or more specified image paths do not exist.\")\n",
    "        exit()\n",
    "\n",
    "    # 3. Load and Preprocess Images\n",
    "    print(\"Loading and preprocessing images...\")\n",
    "    images_tensors = []\n",
    "    focal_lengths = []\n",
    "    original_images_for_vis_and_pc = [] # Keep original images for visualization/point cloud\n",
    "\n",
    "    for image_path in image_paths:\n",
    "        # Load image and focal length using depth_pro function\n",
    "        image_np, _, f_px = load_rgb(image_path)\n",
    "        original_images_for_vis_and_pc.append(image_np) # Store original numpy image\n",
    "\n",
    "        # Apply transform and move to device\n",
    "        image_tensor = transform(image_np).to(device)\n",
    "        images_tensors.append(image_tensor)\n",
    "        focal_lengths.append(f_px)\n",
    "    print(f\"Loaded {len(images_tensors)} images.\")\n",
    "\n",
    "    # 4. Run Depth Inference\n",
    "    print(\"Running depth inference...\")\n",
    "    depth_maps, focal_px_values = run_depth_inference(model, images_tensors, focal_lengths)\n",
    "    print(\"Depth inference complete.\")\n",
    "\n",
    "    # 5. Create Point Clouds\n",
    "    print(\"Creating point clouds...\")\n",
    "    # Use the original numpy images loaded earlier for coloring\n",
    "    point_clouds = create_point_clouds(depth_maps, original_images_for_vis_and_pc, focal_px_values)\n",
    "    print(f\"Created {len(point_clouds)} point clouds.\")\n",
    "\n",
    "    # 6. Save Point Clouds\n",
    "    print(\"Saving point clouds...\")\n",
    "    save_point_clouds(point_clouds, image_paths)\n",
    "    print(\"Point clouds saved.\")\n",
    "\n",
    "    # 7. Optional: Visualize Results\n",
    "    # Ensure images are in the correct format for matplotlib (e.g., numpy array [0,1])\n",
    "    vis_images = [ensure_image_format(img) for img in original_images_for_vis_and_pc]\n",
    "    visualize_results(vis_images, depth_maps)\n",
    "    print(\"Visualization displayed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Point cloud from video with custom focal length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Video: depth maps/Video_1_L_rectified_src.mp4\n",
      "Processing Resolution: 854x480\n",
      "Individual Point Clouds Save Dir: generated_pointclouds/frames_480p\n",
      "Output Video: generated_pointclouds/point_cloud_video_480p.mp4 (854x480 @ 10fps)\n",
      "Processing every 5 frames.\n",
      "Using original average focal length: 1069.53 px (Note: Frames will be resized)\n",
      "GPU (MPS) is available and will be used\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ml-depth-pro/checkpoints/depth_pro.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 146\u001b[0m\n\u001b[1;32m    137\u001b[0m DEFAULT_MONODEPTH_CONFIG_DICT \u001b[38;5;241m=\u001b[39m DepthProConfig( \u001b[38;5;66;03m# Use src.depth_pro\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     patch_encoder_preset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdinov2l16_384\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    139\u001b[0m     image_encoder_preset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdinov2l16_384\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    143\u001b[0m     fov_encoder_preset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdinov2l16_384\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    144\u001b[0m )\n\u001b[1;32m    145\u001b[0m device \u001b[38;5;241m=\u001b[39m setup_device()\n\u001b[0;32m--> 146\u001b[0m model, transform \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model_and_transforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEFAULT_MONODEPTH_CONFIG_DICT\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Use src.depth_pro\u001b[39;00m\n\u001b[1;32m    147\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    148\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/OMGrab/ml-depth-pro/src/depth_pro/depth_pro.py:135\u001b[0m, in \u001b[0;36mcreate_model_and_transforms\u001b[0;34m(config, device, precision)\u001b[0m\n\u001b[1;32m    125\u001b[0m transform \u001b[38;5;241m=\u001b[39m Compose(\n\u001b[1;32m    126\u001b[0m     [\n\u001b[1;32m    127\u001b[0m         ToTensor(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    131\u001b[0m     ]\n\u001b[1;32m    132\u001b[0m )\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mcheckpoint_uri \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     missing_keys, unexpected_keys \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mload_state_dict(\n\u001b[1;32m    137\u001b[0m         state_dict\u001b[38;5;241m=\u001b[39mstate_dict, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     )\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unexpected_keys) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.12/site-packages/torch/serialization.py:1425\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1423\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1425\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1427\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1428\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1429\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1430\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.12/site-packages/torch/serialization.py:751\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    750\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 751\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.12/site-packages/torch/serialization.py:732\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 732\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ml-depth-pro/checkpoints/depth_pro.pt'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./ml-depth-pro')\n",
    "from src.depth_pro.depth_pro import *\n",
    "from src.depth_pro.utils import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import open3d as o3d\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import glob # Needed for finding saved files\n",
    "import time  # Optional: for timing\n",
    "\n",
    "# --- Function Definitions (Assuming setup_device, create_point_clouds, etc. are here) ---\n",
    "def setup_device():\n",
    "    # ... (implementation from previous versions) ...\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"GPU (MPS) is available and will be used\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"GPU (CUDA) is available and will be used\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"GPU is not available, using CPU\")\n",
    "    return device\n",
    "\n",
    "def create_point_clouds(depth_maps, images, focal_px_values, scale_ratio=1.0):\n",
    "    # ... (implementation from previous versions - check if it needs cx, cy based on new H, W) ...\n",
    "    point_clouds = []\n",
    "    for depth_map, image, focal_px in zip(depth_maps, images, focal_px_values):\n",
    "        # image should be the resized image (H, W, 3) numpy array\n",
    "        # depth_map should correspond to the resized image dimensions\n",
    "        H, W = depth_map.shape # Get dimensions from depth map (should match resized image)\n",
    "\n",
    "        # Ensure image is numpy (it should be already after cv2.resize and cvtColor)\n",
    "        if isinstance(image, torch.Tensor):\n",
    "           image = image.cpu().numpy() # Fallback\n",
    "        if image.shape[0] != H or image.shape[1] != W:\n",
    "            print(f\"Warning: Mismatch between depth map ({H}x{W}) and image ({image.shape[0]}x{image.shape[1]}) dimensions in create_point_clouds. Resizing image.\")\n",
    "            image = cv2.resize(image, (W, H)) # Resize color image to match depth map if necessary\n",
    "\n",
    "        # Normalize image colors if they are not 0-1\n",
    "        if image.max() > 1.0:\n",
    "             image = image.astype(np.float32) / 255.0\n",
    "        image = np.clip(image, 0, 1)\n",
    "\n",
    "\n",
    "        # Calculate cx, cy based on the *actual* dimensions being used\n",
    "        cx, cy = W / 2, H / 2\n",
    "\n",
    "        x_grid, y_grid = np.meshgrid(np.arange(W), np.arange(H))\n",
    "\n",
    "        Z = depth_map * scale_ratio\n",
    "        # Need to handle division by zero if focal_px is 0 or Z is 0\n",
    "        # Avoid division by zero for X, Y calculation where Z=0\n",
    "        valid_depth_mask = Z > 1e-6 # Small epsilon to avoid division issues\n",
    "        X = np.zeros_like(Z)\n",
    "        Y = np.zeros_like(Z)\n",
    "\n",
    "        X[valid_depth_mask] = (x_grid[valid_depth_mask] - cx) * Z[valid_depth_mask] / focal_px\n",
    "        Y[valid_depth_mask] = (y_grid[valid_depth_mask] - cy) * Z[valid_depth_mask] / focal_px\n",
    "\n",
    "\n",
    "        point_map = np.stack((X, Y, Z), axis=-1)\n",
    "\n",
    "        point_cloud = o3d.geometry.PointCloud()\n",
    "        # Reshape carefully, ensure image colors match points\n",
    "        points_vec = point_map.reshape(-1, 3)\n",
    "        colors_vec = image.reshape(-1, 3)\n",
    "\n",
    "        # Filter points based on the valid depth mask used earlier\n",
    "        valid_points_flat = valid_depth_mask.flatten()\n",
    "        point_cloud.points = o3d.utility.Vector3dVector(points_vec[valid_points_flat])\n",
    "        point_cloud.colors = o3d.utility.Vector3dVector(colors_vec[valid_points_flat])\n",
    "\n",
    "        # Original Z > 0 check - redundant if valid_depth_mask is used correctly, but safe to keep\n",
    "        # valid_points = Z.flatten() > 0 # Z is already scaled\n",
    "        # point_cloud.points = o3d.utility.Vector3dVector(np.asarray(point_cloud.points)[valid_points])\n",
    "        # point_cloud.colors = o3d.utility.Vector3dVector(np.asarray(point_cloud.colors)[valid_points])\n",
    "\n",
    "        point_clouds.append(point_cloud)\n",
    "    return point_clouds\n",
    "# ... (other functions if needed) ...\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Configuration ---\n",
    "    INPUT_VIDEO_PATH = \"depth maps/Video_1_L_rectified_src.mp4\"\n",
    "    FRAME_INTERVAL = 5 # Process one frame every FRAME_INTERVAL frames\n",
    "    TARGET_HEIGHT = 480 # Target height for processing\n",
    "    TARGET_WIDTH = 854 # Target width for processing\n",
    "\n",
    "    POINT_CLOUD_SAVE_DIR = f\"generated_pointclouds/frames_{TARGET_HEIGHT}p\" # Add resolution to folder name\n",
    "    OUTPUT_VIDEO_FILENAME = f\"point_cloud_video_{TARGET_HEIGHT}p.mp4\" # Add resolution to filename\n",
    "    OUTPUT_VIDEO_DIR = \"generated_pointclouds\" # Where the final video goes\n",
    "\n",
    "    POINT_CLOUD_SCALE_RATIO = 1.0 # Should be 1.0 if f_px is provided to infer\n",
    "    # Output video dimensions should match the processing dimensions\n",
    "    VIS_WIDTH = TARGET_WIDTH\n",
    "    VIS_HEIGHT = TARGET_HEIGHT\n",
    "    FPS = 10 # Frame rate of the output video\n",
    "\n",
    "    # Ensure output directories exist\n",
    "    os.makedirs(POINT_CLOUD_SAVE_DIR, exist_ok=True)\n",
    "    os.makedirs(OUTPUT_VIDEO_DIR, exist_ok=True)\n",
    "\n",
    "    print(f\"Input Video: {INPUT_VIDEO_PATH}\")\n",
    "    print(f\"Processing Resolution: {TARGET_WIDTH}x{TARGET_HEIGHT}\")\n",
    "    print(f\"Individual Point Clouds Save Dir: {POINT_CLOUD_SAVE_DIR}\")\n",
    "    print(f\"Output Video: {os.path.join(OUTPUT_VIDEO_DIR, OUTPUT_VIDEO_FILENAME)} ({VIS_WIDTH}x{VIS_HEIGHT} @ {FPS}fps)\")\n",
    "    print(f\"Processing every {FRAME_INTERVAL} frames.\")\n",
    "\n",
    "    # Define Camera Intrinsics - NOTE: These are for the ORIGINAL resolution.\n",
    "    # If resizing, intrinsics need to be scaled accordingly if used precisely.\n",
    "    # However, we are passing the average f_px to model.infer, which the model might\n",
    "    # handle internally or override. The create_point_clouds function uses the average f_px.\n",
    "    # Scaling f_px and c_x, c_y might be needed for perfect geometric accuracy\n",
    "    # if create_point_clouds were using cx, cy directly from a scaled matrix.\n",
    "    camera_intrinsics_orig = np.array([\n",
    "        [1.06936178e+03, 0.00000000e+00, 5.55299273e+02],\n",
    "        [0.00000000e+00, 1.06969222e+03, 9.44910636e+02],\n",
    "        [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]\n",
    "    ])\n",
    "    fx_orig = camera_intrinsics_orig[0, 0]\n",
    "    fy_orig = camera_intrinsics_orig[1, 1]\n",
    "    # Calculate an average focal length. If the model adjusts based on input aspect ratio,\n",
    "    # this might be sufficient. For precise geometry, scaling fx/fy separately is better.\n",
    "    focal_length_orig_px = (fx_orig + fy_orig) / 2.0\n",
    "    print(f\"Using original average focal length: {focal_length_orig_px:.2f} px (Note: Frames will be resized)\")\n",
    "    # We will pass this focal length value to model.infer and create_point_clouds.\n",
    "    # The model might interpret this f_px relative to the input image size/aspect ratio.\n",
    "\n",
    "    # --- Model and Device Setup ---\n",
    "    # ... (model loading remains the same) ...\n",
    "    DEFAULT_MONODEPTH_CONFIG_DICT = DepthProConfig( # Use src.depth_pro\n",
    "        patch_encoder_preset=\"dinov2l16_384\",\n",
    "        image_encoder_preset=\"dinov2l16_384\",\n",
    "        checkpoint_uri=\"ml-depth-pro/checkpoints/depth_pro.pt\",\n",
    "        decoder_features=256,\n",
    "        use_fov_head=True,\n",
    "        fov_encoder_preset=\"dinov2l16_384\",\n",
    "    )\n",
    "    device = setup_device()\n",
    "    model, transform = create_model_and_transforms(config=DEFAULT_MONODEPTH_CONFIG_DICT) # Use src.depth_pro\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    print(\"Model loaded and moved to device.\")\n",
    "\n",
    "\n",
    "    # ===========================================================\n",
    "    # Phase 1: Generate and Save Individual Point Clouds (with resizing)\n",
    "    # ===========================================================\n",
    "    print(\"\\n--- Starting Phase 1: Generating and Saving Point Clouds ---\")\n",
    "    start_time_phase1 = time.time()\n",
    "\n",
    "    # ... (video file checks remain the same) ...\n",
    "    if not os.path.exists(INPUT_VIDEO_PATH):\n",
    "        print(f\"Error: Video file not found at {INPUT_VIDEO_PATH}\")\n",
    "        exit()\n",
    "    cap = cv2.VideoCapture(INPUT_VIDEO_PATH)\n",
    "    print(f\"number of frames: {cap.get(cv2.CAP_PROP_FRAME_COUNT)}\")\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file {INPUT_VIDEO_PATH}\")\n",
    "        exit()\n",
    "\n",
    "\n",
    "    frame_count = 0\n",
    "    processed_frame_count = 0\n",
    "    saved_files_count = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break # End of video\n",
    "\n",
    "        if frame_count % FRAME_INTERVAL == 0:\n",
    "            print(f\"Processing frame {frame_count} for point cloud saving...\")\n",
    "            processed_frame_count += 1\n",
    "\n",
    "            # <<< Resize Frame >>>\n",
    "            frame_resized = cv2.resize(frame, (TARGET_WIDTH, TARGET_HEIGHT), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            # 1. Preprocess *Resized* Frame\n",
    "            frame_rgb_np = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
    "            image_tensor = transform(frame_rgb_np).to(device)\n",
    "            # Pass the *original* average focal length. The model's `infer` might adapt.\n",
    "            f_px_tensor = torch.tensor(focal_length_orig_px, dtype=torch.float32, device=device)\n",
    "\n",
    "            # 2. Run Depth Inference\n",
    "            with torch.no_grad():\n",
    "                # Model's infer function should handle resizing internally and\n",
    "                # return depth map corresponding to the input tensor's size (resized frame)\n",
    "                prediction = model.infer(image_tensor, f_px=f_px_tensor)\n",
    "                depth_map = prediction[\"depth\"].squeeze().cpu().numpy()\n",
    "\n",
    "            # Check if depth map dimensions match target size\n",
    "            if depth_map.shape[0] != TARGET_HEIGHT or depth_map.shape[1] != TARGET_WIDTH:\n",
    "                 print(f\"Warning: Depth map size {depth_map.shape} doesn't match target {TARGET_HEIGHT}x{TARGET_WIDTH}. Resizing depth map.\")\n",
    "                 depth_map = cv2.resize(depth_map, (TARGET_WIDTH, TARGET_HEIGHT), interpolation=cv2.INTER_NEAREST) # Use nearest for depth\n",
    "\n",
    "\n",
    "            # 3. Create Point Cloud for this frame (using resized image and corresponding depth)\n",
    "            frame_point_clouds = create_point_clouds(\n",
    "                [depth_map],          # Depth map corresponding to resized frame\n",
    "                [frame_rgb_np],       # Resized RGB frame (as numpy)\n",
    "                [focal_length_orig_px], # Original average focal length (for scaling)\n",
    "                scale_ratio=POINT_CLOUD_SCALE_RATIO\n",
    "            )\n",
    "\n",
    "            # 4. Save the Point Cloud if valid\n",
    "            # ... (saving logic remains the same) ...\n",
    "            if frame_point_clouds and frame_point_clouds[0].has_points():\n",
    "                pcd = frame_point_clouds[0]\n",
    "                if pcd.has_points():\n",
    "                    filename = f\"frame_{frame_count:05d}.ply\"\n",
    "                    output_path = os.path.join(POINT_CLOUD_SAVE_DIR, filename)\n",
    "                    o3d.io.write_point_cloud(output_path, pcd)\n",
    "                    saved_files_count += 1\n",
    "                else:\n",
    "                     print(f\"Frame {frame_count}: Point cloud empty after processing/filtering.\")\n",
    "            else:\n",
    "                 print(f\"Frame {frame_count}: No point cloud generated.\")\n",
    "\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    # ... (rest of Phase 1 cleanup remains the same) ...\n",
    "    cap.release()\n",
    "    end_time_phase1 = time.time()\n",
    "    print(f\"--- Phase 1 Complete ({end_time_phase1 - start_time_phase1:.2f} seconds) ---\")\n",
    "    print(f\"Processed {processed_frame_count} frames, saved {saved_files_count} point cloud files.\")\n",
    "\n",
    "    # ... (check for saved_files_count remains the same) ...\n",
    "\n",
    "\n",
    "    # ===========================================================\n",
    "    # Phase 2: Create Video from Saved Point Clouds\n",
    "    # ===========================================================\n",
    "    print(\"\\n--- Starting Phase 2: Creating Video from Saved Point Clouds ---\")\n",
    "    start_time_phase2 = time.time()\n",
    "\n",
    "    # Find all saved .ply files from the correct directory\n",
    "    ply_files = sorted(glob.glob(os.path.join(POINT_CLOUD_SAVE_DIR, \"frame_*.ply\")))\n",
    "\n",
    "    # ... (check if ply_files list is empty remains the same) ...\n",
    "    if not ply_files:\n",
    "        print(f\"Error: No .ply files found in {POINT_CLOUD_SAVE_DIR}. Cannot create video.\")\n",
    "        exit()\n",
    "\n",
    "\n",
    "    # --- Video Output Setup (using VIS_WIDTH, VIS_HEIGHT which match TARGET_WIDTH, TARGET_HEIGHT) ---\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v') # Or 'XVID' for .avi\n",
    "    output_video_path = os.path.join(OUTPUT_VIDEO_DIR, OUTPUT_VIDEO_FILENAME)\n",
    "    # Use VIS_WIDTH, VIS_HEIGHT for VideoWriter size\n",
    "    out_video = cv2.VideoWriter(output_video_path, fourcc, FPS, (VIS_WIDTH, VIS_HEIGHT))\n",
    "    print(f\"Output video will be saved to: {output_video_path}\")\n",
    "\n",
    "    # --- Open3D Visualizer Setup (using VIS_WIDTH, VIS_HEIGHT) ---\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    # Use VIS_WIDTH, VIS_HEIGHT for Visualizer window size\n",
    "    vis.create_window(width=VIS_WIDTH, height=VIS_HEIGHT, visible=False) # Hidden window\n",
    "    # ... (setting background color, getting view control remains the same) ...\n",
    "    render_options = vis.get_render_option()\n",
    "    render_options.background_color = np.asarray([0.0, 0.0, 0.0])\n",
    "    view_control = vis.get_view_control()\n",
    "\n",
    "    # ... (setting initial view based on first cloud remains the same) ...\n",
    "    try:\n",
    "        first_pcd = o3d.io.read_point_cloud(ply_files[0])\n",
    "        points = np.asarray(first_pcd.points)\n",
    "        if points.size > 0:\n",
    "             median_z = np.median(points[:, 2])\n",
    "        else:\n",
    "             median_z = 5.0\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not read first point cloud {ply_files[0]} to set initial view: {e}\")\n",
    "        median_z = 5.0\n",
    "    print(f\"Setting initial view: lookat=[0, 0, {median_z:.2f}], front=[0, 0, 1], up=[0, -1, 0]\")\n",
    "    view_control.set_front([0, 0, 1])\n",
    "    view_control.set_lookat([0, 0, median_z])\n",
    "    view_control.set_up([0, -1, 0])\n",
    "    view_control.set_zoom(0.4) # Adjust zoom if needed\n",
    "\n",
    "\n",
    "    # --- Loop through saved files and render ---\n",
    "    print(f\"Rendering {len(ply_files)} point clouds for video...\")\n",
    "    # ... (looping, reading ply, rendering, writing frame remains the same) ...\n",
    "    for i, ply_file in enumerate(ply_files):\n",
    "        print(f\"Rendering file {i+1}/{len(ply_files)}: {os.path.basename(ply_file)}\")\n",
    "        try:\n",
    "            pcd = o3d.io.read_point_cloud(ply_file)\n",
    "            if not pcd.has_points():\n",
    "                print(\"  ...Skipping empty point cloud.\")\n",
    "                frame_bgr = np.zeros((VIS_HEIGHT, VIS_WIDTH, 3), dtype=np.uint8)\n",
    "            else:\n",
    "                vis.clear_geometries()\n",
    "                vis.add_geometry(pcd)\n",
    "                vis.poll_events()\n",
    "                vis.update_renderer()\n",
    "                o3d_img = vis.capture_screen_float_buffer(do_render=True)\n",
    "                frame_np = (np.asarray(o3d_img) * 255).astype(np.uint8)\n",
    "                frame_bgr = cv2.cvtColor(frame_np, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Error reading or rendering file {ply_file}: {e}\")\n",
    "            frame_bgr = np.zeros((VIS_HEIGHT, VIS_WIDTH, 3), dtype=np.uint8)\n",
    "\n",
    "        out_video.write(frame_bgr)\n",
    "\n",
    "\n",
    "    # --- Cleanup ---\n",
    "    # ... (releasing video, destroying windows remains the same) ...\n",
    "    out_video.release()\n",
    "    vis.destroy_window()\n",
    "    cv2.destroyAllWindows()\n",
    "    end_time_phase2 = time.time()\n",
    "    print(f\"--- Phase 2 Complete ({end_time_phase2 - start_time_phase2:.2f} seconds) ---\")\n",
    "    print(f\"Output video saved to: {output_video_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate video only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 429 point cloud files in /Users/hendrik/OMGrab/point_clouds.\n",
      "Output video: generated_pointclouds/point_cloud_video_set_lookat_method.mp4 (1080x1920 @ 15fps)\n",
      "Open3D visualizer created.\n",
      "Camera orientation matrix (Cam axes in World coords):\n",
      " [[-1.00000000e+00 -1.22464680e-16  0.00000000e+00]\n",
      " [ 8.65956056e-17 -7.07106781e-01  7.07106781e-01]\n",
      " [-8.65956056e-17  7.07106781e-01  7.07106781e-01]]\n",
      "Calculated Camera Eye: [ 0. -3.  0.]\n",
      "Calculated Camera Lookat: [ 0.         -2.29289322  0.70710678]\n",
      "Calculated Camera Front: [0.         0.70710678 0.70710678]\n",
      "Calculated Camera Up: [ 1.22464680e-16  7.07106781e-01 -7.07106781e-01]\n",
      "Camera view parameters set using set_lookat/front/up.\n",
      "Rendering 429 point clouds...\n",
      "  Processing file 50/429: frame_00049.ply\n",
      "  Processing file 100/429: frame_00099.ply\n",
      "  Processing file 150/429: frame_00149.ply\n",
      "  Processing file 200/429: frame_00199.ply\n",
      "  Processing file 250/429: frame_00249.ply\n",
      "  Processing file 300/429: frame_00299.ply\n",
      "  Processing file 350/429: frame_00349.ply\n",
      "  Processing file 400/429: frame_00399.ply\n",
      "\n",
      "Rendering complete in 15.33 seconds.\n",
      "Output video saved to: generated_pointclouds/point_cloud_video_set_lookat_method.mp4\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "\n",
    "# --- Configuration ---\n",
    "POINT_CLOUD_DIR = \"/Users/hendrik/OMGrab/point_clouds\"\n",
    "OUTPUT_VIDEO_FILENAME = \"point_cloud_video_set_lookat_method.mp4\" # New filename\n",
    "OUTPUT_VIDEO_DIR = \"generated_pointclouds\"\n",
    "\n",
    "VIS_WIDTH = 1080\n",
    "VIS_HEIGHT = 1920\n",
    "FPS = 15\n",
    "\n",
    "CAMERA_ORIGIN = np.array([0.0, -3.0, 0.0])\n",
    "CAMERA_X_ROTATION_DEG = -45.0\n",
    "CAMERA_Z_ROTATION_DEG = 180.0\n",
    "\n",
    "# --- End Configuration ---\n",
    "\n",
    "def create_video_from_point_clouds(\n",
    "    pcd_dir,\n",
    "    output_dir,\n",
    "    output_filename,\n",
    "    width,\n",
    "    height,\n",
    "    fps,\n",
    "    camera_origin,\n",
    "    camera_x_rotation_deg,\n",
    "    camera_z_rotation_deg\n",
    "):\n",
    "    \"\"\"Generates video from point clouds using set_lookat/front/up.\"\"\"\n",
    "    # ... (Setup code: directory checks, file finding, video writer init remains the same) ...\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_video_path = os.path.join(output_dir, output_filename)\n",
    "    ply_files = sorted(glob.glob(os.path.join(pcd_dir, \"frame_*.ply\")))\n",
    "    if not ply_files:\n",
    "        print(f\"Error: No .ply files found in {pcd_dir} matching 'frame_*.ply'.\")\n",
    "        return\n",
    "    print(f\"Found {len(ply_files)} point cloud files in {pcd_dir}.\")\n",
    "    print(f\"Output video: {output_video_path} ({width}x{height} @ {fps}fps)\")\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out_video = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "    if not out_video.isOpened():\n",
    "        print(f\"Error: Could not open video writer for path {output_video_path}\")\n",
    "        return\n",
    "\n",
    "    # --- Open3D Visualizer Setup ---\n",
    "    # ... (Visualizer creation, background color remains the same) ...\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window(width=width, height=height, visible=False)\n",
    "    render_options = vis.get_render_option()\n",
    "    render_options.background_color = np.asarray([0.0, 0.0, 0.0])\n",
    "    print(\"Open3D visualizer created.\")\n",
    "    view_control = vis.get_view_control()\n",
    "\n",
    "    # --- Calculate Camera View Vectors ---\n",
    "    # Convert degrees to radians\n",
    "    theta_x = math.radians(camera_x_rotation_deg)\n",
    "    theta_z = math.radians(camera_z_rotation_deg)\n",
    "\n",
    "    # Rotation matrix for X-axis rotation (around world X)\n",
    "    cos_x, sin_x = math.cos(theta_x), math.sin(theta_x)\n",
    "    R_x = np.array([\n",
    "        [1.0, 0.0,   0.0],\n",
    "        [0.0, cos_x, -sin_x],\n",
    "        [0.0, sin_x,  cos_x]\n",
    "    ])\n",
    "\n",
    "    # Rotation matrix for Z-axis rotation (around world Z)\n",
    "    cos_z, sin_z = math.cos(theta_z), math.sin(theta_z)\n",
    "    R_z = np.array([\n",
    "        [cos_z, -sin_z, 0.0],\n",
    "        [sin_z,  cos_z, 0.0],\n",
    "        [0.0,    0.0,   1.0]\n",
    "    ])\n",
    "\n",
    "    # Combine rotations: Apply Z rotation first, then X rotation in world frame.\n",
    "    # R = Rx * Rz -> This matrix rotates vectors from camera frame to world frame.\n",
    "    R_cam_to_world = R_x @ R_z\n",
    "    print(\"Camera orientation matrix (Cam axes in World coords):\\n\", R_cam_to_world)\n",
    "\n",
    "    # Default camera view vectors in camera's own coordinate system:\n",
    "    #  - points along +Z\n",
    "    #  - 'up' direction corresponds to -Y\n",
    "    default_front_vec = np.array([0.0, 0.0, 1.0])\n",
    "    default_up_vec = np.array([0.0, -1.0, 0.0])\n",
    "\n",
    "    # Rotate these default vectors by the camera's orientation to get them in world coordinates\n",
    "    world_front_vec = R_cam_to_world @ default_front_vec\n",
    "    world_up_vec = R_cam_to_world @ default_up_vec\n",
    "\n",
    "    # The point the camera looks at is the origin plus the direction vector\n",
    "    lookat_point = camera_origin + world_front_vec\n",
    "\n",
    "    print(f\"Calculated Camera Eye: {camera_origin}\")\n",
    "    print(f\"Calculated Camera Lookat: {lookat_point}\")\n",
    "    print(f\"Calculated Camera Front: {world_front_vec}\")\n",
    "    print(f\"Calculated Camera Up: {world_up_vec}\")\n",
    "\n",
    "\n",
    "    # --- Set View using set_lookat/set_front/set_up ---\n",
    "    # These methods might be more reliable for setting orientation\n",
    "    view_control.set_front(world_front_vec)\n",
    "    view_control.set_up(world_up_vec)\n",
    "    view_control.set_lookat(lookat_point) # Center view target\n",
    "    # Eye position is implicitly set based on lookat and front, but we can try setting zoom\n",
    "    view_control.set_zoom(0.6) # Adjust zoom level as needed (try different values)\n",
    "\n",
    "    print(\"Camera view parameters set using set_lookat/front/up.\")\n",
    "\n",
    "    # --- Loop through saved files and render ---\n",
    "    # ... (The loop for reading, rendering, capturing, and writing frames remains the same) ...\n",
    "    print(f\"Rendering {len(ply_files)} point clouds...\")\n",
    "    start_time = time.time()\n",
    "    for i, ply_file in enumerate(ply_files):\n",
    "        if (i+1) % 50 == 0:\n",
    "             print(f\"  Processing file {i+1}/{len(ply_files)}: {os.path.basename(ply_file)}\")\n",
    "        try:\n",
    "            pcd = o3d.io.read_point_cloud(ply_file)\n",
    "            if not pcd.has_points():\n",
    "                print(f\"  Warning: Skipping empty point cloud file {os.path.basename(ply_file)}.\")\n",
    "                frame_bgr = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "            else:\n",
    "                vis.clear_geometries()\n",
    "                vis.add_geometry(pcd)\n",
    "                # Re-apply view parameters inside loop IF NEEDED (usually not)\n",
    "                # view_control.set_front(world_front_vec)\n",
    "                # view_control.set_up(world_up_vec)\n",
    "                # view_control.set_lookat(lookat_point)\n",
    "                vis.poll_events()\n",
    "                vis.update_renderer()\n",
    "                o3d_img = vis.capture_screen_float_buffer(do_render=True)\n",
    "                frame_np = (np.asarray(o3d_img) * 255).astype(np.uint8)\n",
    "                frame_bgr = cv2.cvtColor(frame_np, cv2.COLOR_RGB2BGR)\n",
    "        except Exception as e:\n",
    "            print(f\"  Error reading or rendering file {ply_file}: {e}\")\n",
    "            frame_bgr = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "        out_video.write(frame_bgr)\n",
    "\n",
    "    # --- Cleanup ---\n",
    "    # ... (Releasing video writer, destroying window remains the same) ...\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nRendering complete in {end_time - start_time:.2f} seconds.\")\n",
    "    out_video.release()\n",
    "    vis.destroy_window()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"Output video saved to: {output_video_path}\")\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    create_video_from_point_clouds(\n",
    "        pcd_dir=POINT_CLOUD_DIR,\n",
    "        output_dir=OUTPUT_VIDEO_DIR,\n",
    "        output_filename=OUTPUT_VIDEO_FILENAME,\n",
    "        width=VIS_WIDTH,\n",
    "        height=VIS_HEIGHT,\n",
    "        fps=FPS,\n",
    "        camera_origin=CAMERA_ORIGIN,\n",
    "        camera_x_rotation_deg=CAMERA_X_ROTATION_DEG,\n",
    "        camera_z_rotation_deg=CAMERA_Z_ROTATION_DEG\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 429 point cloud files in /Users/hendrik/OMGrab/point_clouds.\n",
      "Output video: generated_pointclouds/point_cloud_video_stereo_calibrate_method.mp4 (1080x1920 @ 15fps)\n",
      "Open3D visualizer created.\n",
      "Defined point cloud rotation matrix (180 deg around X).\n",
      "Setting fixed camera extrinsic matrix:\n",
      " [[ 1  0  0  0]\n",
      " [ 0 -1  0  0]\n",
      " [ 0  0 -1  0]\n",
      " [ 0  0  0  1]]\n",
      "Camera view parameters set. Zoom level: 0.4\n",
      "Initial visualizer state set.\n",
      "Rendering 429 point clouds...\n",
      "  Processing file 50/429: frame_00049.ply\n",
      "  Processing file 100/429: frame_00099.ply\n",
      "  Processing file 150/429: frame_00149.ply\n",
      "  Processing file 200/429: frame_00199.ply\n",
      "  Processing file 250/429: frame_00249.ply\n",
      "  Processing file 300/429: frame_00299.ply\n",
      "  Processing file 350/429: frame_00349.ply\n",
      "  Processing file 400/429: frame_00399.ply\n",
      "\n",
      "Rendering complete in 15.79 seconds.\n",
      "Output video saved to: generated_pointclouds/point_cloud_video_stereo_calibrate_method.mp4\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "\n",
    "# --- Configuration ---\n",
    "POINT_CLOUD_DIR = \"/Users/hendrik/OMGrab/point_clouds\"\n",
    "OUTPUT_VIDEO_FILENAME = \"point_cloud_video_stereo_calibrate_method.mp4\" # New filename\n",
    "OUTPUT_VIDEO_DIR = \"generated_pointclouds\"\n",
    "\n",
    "VIS_WIDTH = 1080  # Width for visualizer and video frame\n",
    "VIS_HEIGHT = 1920 # Height for visualizer and video frame\n",
    "FPS = 15\n",
    "\n",
    "# Note: Camera origin/rotation parameters are not used in this method,\n",
    "# as the view is set by a fixed extrinsic matrix and point cloud rotation.\n",
    "# CAMERA_ORIGIN = np.array([0.0, -3.0, 0.0])\n",
    "# CAMERA_X_ROTATION_DEG = -45.0\n",
    "# CAMERA_Z_ROTATION_DEG = 180.0\n",
    "\n",
    "# --- End Configuration ---\n",
    "\n",
    "def create_video_from_point_clouds_stereo_method( # Renamed function\n",
    "    pcd_dir,\n",
    "    output_dir,\n",
    "    output_filename,\n",
    "    width,\n",
    "    height,\n",
    "    fps\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates video from point clouds using the method from stereo_calibrate.ipynb.\n",
    "    Rotates point clouds and uses a fixed extrinsic matrix for the camera view.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_video_path = os.path.join(output_dir, output_filename)\n",
    "    ply_files = sorted(glob.glob(os.path.join(pcd_dir, \"frame_*.ply\")))\n",
    "\n",
    "    if not ply_files:\n",
    "        print(f\"Error: No .ply files found in {pcd_dir} matching 'frame_*.ply'.\")\n",
    "        return\n",
    "    print(f\"Found {len(ply_files)} point cloud files in {pcd_dir}.\")\n",
    "    print(f\"Output video: {output_video_path} ({width}x{height} @ {fps}fps)\")\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    # Ensure VideoWriter uses the correct width and height\n",
    "    out_video = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "    if not out_video.isOpened():\n",
    "        print(f\"Error: Could not open video writer for path {output_video_path}\")\n",
    "        return\n",
    "\n",
    "    # --- Open3D Visualizer Setup ---\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    # Ensure visualizer window matches output video dimensions\n",
    "    vis.create_window(width=width, height=height, visible=False)\n",
    "    render_options = vis.get_render_option()\n",
    "    render_options.background_color = np.asarray([0.0, 0.0, 0.0]) # Black background\n",
    "    render_options.point_size = 1.0 # Adjust point size if needed (example used 4.0)\n",
    "    print(\"Open3D visualizer created.\")\n",
    "\n",
    "    # --- Define Point Cloud Rotation ---\n",
    "    # Rotation matrix to rotate point cloud 180 degrees around X-axis\n",
    "    R_pc_fix = o3d.geometry.get_rotation_matrix_from_xyz((np.pi, 0, 0))\n",
    "    print(\"Defined point cloud rotation matrix (180 deg around X).\")\n",
    "\n",
    "    # --- Set Camera View using Fixed Extrinsic Matrix ---\n",
    "    view_control = vis.get_view_control()\n",
    "    pinhole_params = view_control.convert_to_pinhole_camera_parameters()\n",
    "\n",
    "    # Fixed extrinsic matrix from stereo_calibrate example\n",
    "    # Places camera at [0,0,2] looking toward origin, Y-axis flipped in view\n",
    "    fixed_extrinsic = np.array([\n",
    "        [1, 0, 0, 0],\n",
    "        [0, -1, 0, 0],\n",
    "        [0, 0, -1, 0], # Adjust Z offset (last value, '2') to move camera closer/further\n",
    "        [0, 0, 0, 1]\n",
    "    ])\n",
    "    pinhole_params.extrinsic = fixed_extrinsic\n",
    "    print(\"Setting fixed camera extrinsic matrix:\\n\", fixed_extrinsic)\n",
    "\n",
    "    # Apply the modified parameters back to the view control\n",
    "    view_control.convert_from_pinhole_camera_parameters(pinhole_params, allow_arbitrary=True)\n",
    "\n",
    "    # Adjust zoom level (smaller value = more zoom)\n",
    "    zoom_level = 0.4 # Adjust as needed (example used 0.1, which is very zoomed in)\n",
    "    view_control.set_zoom(zoom_level)\n",
    "    print(f\"Camera view parameters set. Zoom level: {zoom_level}\")\n",
    "\n",
    "    # Add first geometry to initialize renderer state (optional but good practice)\n",
    "    try:\n",
    "        pcd_initial = o3d.io.read_point_cloud(ply_files[0])\n",
    "        pcd_initial.rotate(R_pc_fix, center=(0, 0, 0)) # Apply rotation\n",
    "        vis.add_geometry(pcd_initial)\n",
    "        vis.poll_events()\n",
    "        vis.update_renderer()\n",
    "        vis.clear_geometries() # Clear it after setup if looping starts from first file\n",
    "        print(\"Initial visualizer state set.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load or process first point cloud for initial setup: {e}\")\n",
    "\n",
    "\n",
    "    # --- Loop through saved files and render ---\n",
    "    print(f\"Rendering {len(ply_files)} point clouds...\")\n",
    "    start_time = time.time()\n",
    "    for i, ply_file in enumerate(ply_files):\n",
    "        if (i+1) % 50 == 0:\n",
    "             print(f\"  Processing file {i+1}/{len(ply_files)}: {os.path.basename(ply_file)}\")\n",
    "        try:\n",
    "            pcd = o3d.io.read_point_cloud(ply_file)\n",
    "            if not pcd.has_points():\n",
    "                print(f\"  Warning: Skipping empty point cloud file {os.path.basename(ply_file)}.\")\n",
    "                frame_bgr = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "            else:\n",
    "                # <<< Apply the 180-degree X-axis rotation to the point cloud >>>\n",
    "                pcd.rotate(R_pc_fix, center=(0, 0, 0))\n",
    "\n",
    "                vis.clear_geometries()\n",
    "                vis.add_geometry(pcd)\n",
    "\n",
    "                # Camera view is fixed, just update rendering state\n",
    "                vis.poll_events()\n",
    "                vis.update_renderer()\n",
    "\n",
    "                # Capture frame\n",
    "                o3d_img = vis.capture_screen_float_buffer(do_render=True)\n",
    "                frame_np = (np.asarray(o3d_img) * 255).astype(np.uint8)\n",
    "                frame_bgr = cv2.cvtColor(frame_np, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Error reading or rendering file {ply_file}: {e}\")\n",
    "            frame_bgr = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "\n",
    "        out_video.write(frame_bgr)\n",
    "\n",
    "    # --- Cleanup ---\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nRendering complete in {end_time - start_time:.2f} seconds.\")\n",
    "    out_video.release()\n",
    "    vis.destroy_window()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"Output video saved to: {output_video_path}\")\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Call the new function\n",
    "    create_video_from_point_clouds_stereo_method(\n",
    "        pcd_dir=POINT_CLOUD_DIR,\n",
    "        output_dir=OUTPUT_VIDEO_DIR,\n",
    "        output_filename=OUTPUT_VIDEO_FILENAME,\n",
    "        width=VIS_WIDTH,\n",
    "        height=VIS_HEIGHT,\n",
    "        fps=FPS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 429 point cloud files in /Users/hendrik/OMGrab/point_clouds.\n",
      "Output video: generated_pointclouds/point_cloud_video_centered_render.mp4 (1920x1920 @ 15fps)\n",
      "Open3D visualizer created.\n",
      "Defined point cloud rotation matrix (180 deg around X).\n",
      "Using Scaled Focal Lengths: fx=1604.00, fy=2852.25\n",
      "Using Centered Principal Point: cx=960.00, cy=960.00\n",
      "Created rendering intrinsic matrix:\n",
      " [[1.60400138e+03 0.00000000e+00 9.60000000e+02]\n",
      " [0.00000000e+00 2.85224968e+03 9.60000000e+02]\n",
      " [0.00000000e+00 0.00000000e+00 1.00000000e+00]]\n",
      "Setting fixed camera extrinsic matrix:\n",
      " [[ 1.   0.   0.   0. ]\n",
      " [ 0.  -1.   0.   0. ]\n",
      " [ 0.   0.  -1.   0.1]\n",
      " [ 0.   0.   0.   1. ]]\n",
      "Camera view parameters set. Zoom level: 0.9\n",
      "Initial visualizer state set.\n",
      "Rendering 429 point clouds...\n",
      "  Processing file 50/429: frame_00049.ply\n",
      "  Processing file 100/429: frame_00099.ply\n",
      "  Processing file 150/429: frame_00149.ply\n",
      "  Processing file 200/429: frame_00199.ply\n",
      "  Processing file 250/429: frame_00249.ply\n",
      "  Processing file 300/429: frame_00299.ply\n",
      "  Processing file 350/429: frame_00349.ply\n",
      "  Processing file 400/429: frame_00399.ply\n",
      "\n",
      "Rendering complete in 21.22 seconds.\n",
      "Output video saved to: generated_pointclouds/point_cloud_video_centered_render.mp4\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "\n",
    "# --- Configuration ---\n",
    "POINT_CLOUD_DIR = \"/Users/hendrik/OMGrab/point_clouds\"\n",
    "OUTPUT_VIDEO_FILENAME = \"point_cloud_video_centered_render.mp4\" # New filename\n",
    "OUTPUT_VIDEO_DIR = \"generated_pointclouds\"\n",
    "\n",
    "# Resolution the point clouds were likely generated at\n",
    "# Verify this matches the actual generation process\n",
    "GENERATION_WIDTH = 1280\n",
    "GENERATION_HEIGHT = 720\n",
    "\n",
    "# Rendering resolution\n",
    "VIS_WIDTH = 1920\n",
    "VIS_HEIGHT = 1920\n",
    "FPS = 15\n",
    "\n",
    "# --- Intrinsics used during point cloud generation ---\n",
    "# Assuming these correspond to GENERATION_WIDTH x GENERATION_HEIGHT\n",
    "K_gen = np.array([\n",
    "    [1.06933425e+03, 0.00000000e+00, 5.55076352e+02], # fx_gen, 0, cx_gen\n",
    "    [0.00000000e+00, 1.06959363e+03, 9.45074567e+02], # 0, fy_gen, cy_gen\n",
    "    [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]\n",
    "])\n",
    "\n",
    "# --- End Configuration ---\n",
    "\n",
    "def create_video_from_point_clouds_centered_render( # Renamed function\n",
    "    pcd_dir,\n",
    "    output_dir,\n",
    "    output_filename,\n",
    "    render_width,\n",
    "    render_height,\n",
    "    fps,\n",
    "    gen_intrinsics, # Still need fx, fy from generation\n",
    "    gen_width,\n",
    "    gen_height\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates video using stereo_calibrate method, but forces the rendering\n",
    "    principal point to the center of the output frame.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_video_path = os.path.join(output_dir, output_filename)\n",
    "    ply_files = sorted(glob.glob(os.path.join(pcd_dir, \"frame_*.ply\")))\n",
    "\n",
    "    if not ply_files:\n",
    "        print(f\"Error: No .ply files found in {pcd_dir} matching 'frame_*.ply'.\")\n",
    "        return\n",
    "    print(f\"Found {len(ply_files)} point cloud files in {pcd_dir}.\")\n",
    "    print(f\"Output video: {output_video_path} ({render_width}x{render_height} @ {fps}fps)\")\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out_video = cv2.VideoWriter(output_video_path, fourcc, fps, (render_width, render_height))\n",
    "    if not out_video.isOpened():\n",
    "        print(f\"Error: Could not open video writer for path {output_video_path}\")\n",
    "        return\n",
    "\n",
    "    # --- Open3D Visualizer Setup ---\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window(width=render_width, height=render_height, visible=False)\n",
    "    render_options = vis.get_render_option()\n",
    "    render_options.background_color = np.asarray([0.0, 0.0, 0.0])\n",
    "    render_options.point_size = 1.0\n",
    "    print(\"Open3D visualizer created.\")\n",
    "\n",
    "    # --- Define Point Cloud Rotation ---\n",
    "    R_pc_fix = o3d.geometry.get_rotation_matrix_from_xyz((np.pi, 0, 0))\n",
    "    print(\"Defined point cloud rotation matrix (180 deg around X).\")\n",
    "\n",
    "    # --- Calculate SCALED FOCAL LENGTH and CENTERED PRINCIPAL POINT ---\n",
    "    scale_x = render_width / gen_width\n",
    "    scale_y = render_height / gen_height\n",
    "\n",
    "    # Scale focal lengths based on generation intrinsics and render size\n",
    "    fx_render = gen_intrinsics[0, 0] * scale_x\n",
    "    fy_render = gen_intrinsics[1, 1] * scale_y\n",
    "\n",
    "    # FORCE principal point to the center of the rendering window\n",
    "    cx_render = render_width / 2.0\n",
    "    cy_render = render_height / 2.0\n",
    "    print(f\"Using Scaled Focal Lengths: fx={fx_render:.2f}, fy={fy_render:.2f}\")\n",
    "    print(f\"Using Centered Principal Point: cx={cx_render:.2f}, cy={cy_render:.2f}\")\n",
    "\n",
    "\n",
    "    # Create the intrinsic object for the rendering camera\n",
    "    intrinsic_render = o3d.camera.PinholeCameraIntrinsic(\n",
    "        render_width, render_height, fx_render, fy_render, cx_render, cy_render\n",
    "    )\n",
    "    print(\"Created rendering intrinsic matrix:\\n\", intrinsic_render.intrinsic_matrix)\n",
    "\n",
    "\n",
    "    # --- Set Camera View using Intrinsics and Fixed Extrinsic ---\n",
    "    view_control = vis.get_view_control()\n",
    "    pinhole_params = view_control.convert_to_pinhole_camera_parameters()\n",
    "\n",
    "    # Set the calculated intrinsic parameters\n",
    "    pinhole_params.intrinsic = intrinsic_render\n",
    "\n",
    "    # Set the fixed extrinsic matrix (Camera at origin, looking down -Z after pcd rotation)\n",
    "    # Keep small Z offset\n",
    "    fixed_extrinsic = np.array([\n",
    "        [1, 0, 0, 0],\n",
    "        [0, -1, 0, 0],\n",
    "        [0, 0, -1, 0.1], # Small Z offset\n",
    "        [0, 0, 0, 1]\n",
    "    ])\n",
    "    pinhole_params.extrinsic = fixed_extrinsic\n",
    "    print(\"Setting fixed camera extrinsic matrix:\\n\", fixed_extrinsic)\n",
    "\n",
    "    # Apply the modified parameters back to the view control\n",
    "    view_control.convert_from_pinhole_camera_parameters(pinhole_params, allow_arbitrary=True)\n",
    "\n",
    "    # Adjust zoom level - start closer to 1.0 now that view should be centered\n",
    "    zoom_level = 0.9 # Larger value = less zoom. Adjust if needed.\n",
    "    view_control.set_zoom(zoom_level)\n",
    "    print(f\"Camera view parameters set. Zoom level: {zoom_level}\")\n",
    "\n",
    "    # ... (Initialize visualizer state with first frame - remains the same) ...\n",
    "    try:\n",
    "        pcd_initial = o3d.io.read_point_cloud(ply_files[0])\n",
    "        pcd_initial.rotate(R_pc_fix, center=(0, 0, 0))\n",
    "        vis.add_geometry(pcd_initial)\n",
    "        vis.poll_events()\n",
    "        vis.update_renderer()\n",
    "        vis.clear_geometries()\n",
    "        print(\"Initial visualizer state set.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load or process first point cloud for initial setup: {e}\")\n",
    "\n",
    "\n",
    "    # --- Loop through saved files and render ---\n",
    "    print(f\"Rendering {len(ply_files)} point clouds...\")\n",
    "    start_time = time.time()\n",
    "    # ... (The loop structure remains the same) ...\n",
    "    for i, ply_file in enumerate(ply_files):\n",
    "        if (i+1) % 50 == 0:\n",
    "             print(f\"  Processing file {i+1}/{len(ply_files)}: {os.path.basename(ply_file)}\")\n",
    "        try:\n",
    "            pcd = o3d.io.read_point_cloud(ply_file)\n",
    "            if not pcd.has_points():\n",
    "                print(f\"  Warning: Skipping empty point cloud file {os.path.basename(ply_file)}.\")\n",
    "                frame_bgr = np.zeros((render_height, render_width, 3), dtype=np.uint8)\n",
    "            else:\n",
    "                # Apply the 180-degree X-axis rotation to the point cloud\n",
    "                pcd.rotate(R_pc_fix, center=(0, 0, 0))\n",
    "\n",
    "                vis.clear_geometries()\n",
    "                vis.add_geometry(pcd)\n",
    "\n",
    "                # Camera view is fixed by parameters set outside the loop\n",
    "                vis.poll_events()\n",
    "                vis.update_renderer()\n",
    "\n",
    "                # Capture frame\n",
    "                o3d_img = vis.capture_screen_float_buffer(do_render=True)\n",
    "                frame_np = (np.asarray(o3d_img) * 255).astype(np.uint8)\n",
    "                frame_bgr = cv2.cvtColor(frame_np, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Error reading or rendering file {ply_file}: {e}\")\n",
    "            frame_bgr = np.zeros((render_height, render_width, 3), dtype=np.uint8)\n",
    "\n",
    "        out_video.write(frame_bgr)\n",
    "\n",
    "    # --- Cleanup ---\n",
    "    # ... (Cleanup remains the same) ...\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nRendering complete in {end_time - start_time:.2f} seconds.\")\n",
    "    out_video.release()\n",
    "    vis.destroy_window()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"Output video saved to: {output_video_path}\")\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Call the renamed function\n",
    "    create_video_from_point_clouds_centered_render(\n",
    "        pcd_dir=POINT_CLOUD_DIR,\n",
    "        output_dir=OUTPUT_VIDEO_DIR,\n",
    "        output_filename=OUTPUT_VIDEO_FILENAME,\n",
    "        render_width=VIS_WIDTH,\n",
    "        render_height=VIS_HEIGHT,\n",
    "        fps=FPS,\n",
    "        gen_intrinsics=K_gen,\n",
    "        gen_width=GENERATION_WIDTH,\n",
    "        gen_height=GENERATION_HEIGHT\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
